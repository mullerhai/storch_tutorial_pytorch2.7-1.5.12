{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e65e6ce3",
   "metadata": {},
   "source": [
    "# 手撕Transformer-小冬瓜AIGC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861a9b75",
   "metadata": {},
   "source": [
    "![contetn](image/content.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d19b90",
   "metadata": {},
   "source": [
    "## 1 预处理requirements/configure/tokenizer/dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff511685",
   "metadata": {},
   "source": [
    "### 1.1 requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0934d833",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torchtext==0.0.6\n",
    "!pip3 install spacy\n",
    "!pip3 install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463da99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy download de_core_news_sm\n",
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4cedcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import spacy\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.optim import Adam\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54002aa",
   "metadata": {},
   "source": [
    "### 1.2 configure配置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fc6ea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 配置参数\n",
    "# GPU device setting\n",
    " \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 模型参数\n",
    "batch_size = 128 # 训练批次 句话\n",
    "max_len = 256    # 单句最大长度 \n",
    "##\n",
    "# padding=10\n",
    "\n",
    "d_model = 512    # 词嵌入向量维度\n",
    "n_layers = 6     # encoder/decoder层数量\n",
    "n_heads = 8      # 注意力头数： 假如有词嵌入维度d_model = 512 / n_heads = 8 => 单头向量维度 512 / 8 = 64，即QKV维度\n",
    "ffn_hidden = 2048 # 前向传播维度。 512 -> 2048 -> 512, 通常也称作proj\n",
    "drop_prob = 0.1  # dropout提升鲁棒性，随机失活一些节点\n",
    "n_hidden = ffn_hidden\n",
    "\n",
    "# optimizer parameter setting\n",
    "init_lr = 1e-5\n",
    "factor = 0.9\n",
    "adam_eps = 5e-9\n",
    "patience = 10\n",
    "warmup = 100\n",
    "epoch = 100\n",
    "clip = 1.0\n",
    "weight_decay = 5e-4\n",
    "inf = float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a35dc0",
   "metadata": {},
   "source": [
    "### 1.3 Tokenizer 英德文tokenzier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ba38b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example sentence.\n",
      "['This', 'is', 'an', 'example', 'sentence', '.']\n"
     ]
    }
   ],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.spacy_de = spacy.load('de_core_news_sm')\n",
    "        self.spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "    def tokenize_de(self, text):\n",
    "        return [tok.text for tok in self.spacy_de.tokenizer(text)]\n",
    "\n",
    "    def tokenize_en(self, text):\n",
    "        return [tok.text for tok in self.spacy_en.tokenizer(text)]\n",
    "        # example\n",
    "        # doc = nlp('This is an example sentence.')\n",
    "        # tokens = [token.text for token in doc]\n",
    "        # print(tokens)\n",
    "        # ['This', 'is', 'an', 'example', 'sentence', '.']\n",
    "\n",
    "# 加载Token\n",
    "tokenizer = Tokenizer()\n",
    "example = 'This is an example sentence.'\n",
    "tokens = tokenizer.tokenize_en(example)\n",
    "# tokenizer将句子按照单词分成list\n",
    "print(example)\n",
    "print(tokens)\n",
    "# ['This', 'is', 'an', 'example', 'sentence', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc0d5bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two young, white males are outside near many bushes\n",
      "['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes']\n"
     ]
    }
   ],
   "source": [
    "example = 'two young, white males are outside near many bushes'\n",
    "tokens = tokenizer.tokenize_en(example)\n",
    "print(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e0d071",
   "metadata": {},
   "source": [
    "### 1.4 Dataloader创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e843f242",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset initializing start\n",
      "\n",
      "--------0. 根据spacy mutli30k 创建数据集-------\n",
      "['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']\n",
      "['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.']\n",
      "29000\n",
      "1000\n",
      "1014\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import Field, BucketIterator\n",
    "from torchtext.datasets.translation import Multi30k\n",
    "class DataLoader:\n",
    "    source: Field = None\n",
    "    target: Field = None\n",
    "    def __init__(self, ext, tokenize_en, tokenize_de, init_token, eos_token):\n",
    "        self.ext = ext\n",
    "        self.tokenize_en = tokenize_en\n",
    "        self.tokenize_de = tokenize_de\n",
    "        self.init_token = init_token\n",
    "        self.eos_token = eos_token\n",
    "        print('dataset initializing start')\n",
    "\n",
    "    def make_dataset(self):\n",
    "        if self.ext == ('.de', '.en'):\n",
    "            self.source = Field(tokenize=self.tokenize_de, init_token=self.init_token, eos_token=self.eos_token,\n",
    "                                lower=True, batch_first=True)\n",
    "            self.target = Field(tokenize=self.tokenize_en, init_token=self.init_token, eos_token=self.eos_token,\n",
    "                                lower=True, batch_first=True)\n",
    "\n",
    "        elif self.ext == ('.en', '.de'):\n",
    "            # Field() 函数返回一个 Field 类的实例，该实例有以下常用方法\n",
    "            # build_vocab：根据数据集构建词汇表。\n",
    "            self.source = Field(tokenize=self.tokenize_en, init_token=self.init_token, eos_token=self.eos_token,\n",
    "                                lower=True, batch_first=True)\n",
    "            self.target = Field(tokenize=self.tokenize_de, init_token=self.init_token, eos_token=self.eos_token,\n",
    "                                lower=True, batch_first=True)\n",
    "        # 拆分数据集\n",
    "        train_data, valid_data, test_data = Multi30k.splits(exts=self.ext, fields=(self.source, self.target))\n",
    "        return train_data, valid_data, test_data\n",
    "\n",
    "    def build_vocab(self, train_data, min_freq):\n",
    "        self.source.build_vocab(train_data, min_freq=min_freq)\n",
    "        self.target.build_vocab(train_data, min_freq=min_freq)\n",
    "\n",
    "    def make_iter(self, train, validate, test, batch_size, device):\n",
    "        train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train, validate, test),\n",
    "                                                                              batch_size=batch_size,\n",
    "                                                                              device=device)\n",
    "        print('dataset initializing done')\n",
    "        return train_iterator, valid_iterator, test_iterator\n",
    "\n",
    "# 需要对整句加上句头句尾token [<sos>, 'This', 'is', 'an', 'example', 'sentence', '.',  <eos>] \n",
    "loader = DataLoader(ext=('.en', '.de'),\n",
    "                    tokenize_en=tokenizer.tokenize_en,\n",
    "                    tokenize_de=tokenizer.tokenize_de,\n",
    "                    init_token='<sos>',\n",
    "                    eos_token='<eos>')\n",
    "\n",
    "# 创建 source/target Field实例（包含数据）\n",
    "print('\\n--------0. 根据spacy mutli30k 创建数据集-------')\n",
    "train, valid, test = loader.make_dataset()\n",
    "print(train.examples[0].src)\n",
    "print(train.examples[0].trg)\n",
    "print(len(train.examples))\n",
    "print(len(test.examples))\n",
    "print(len(valid.examples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99ad58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader.build_vocab(train_data=train, min_freq=2)\n",
    "print('--------1. 查看词表大小-------')\n",
    "print('src vocab size:', len(loader.source.vocab.stoi)) \n",
    "print('trg vocab size:', len(loader.target.vocab.stoi)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd0ce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--------2. 建立词表后，如何将单词转成token数值-------')\n",
    "# print('查看词表:', loader.source.vocab.stoi)\n",
    "print('word \\t -> \\t token')\n",
    "print('<sos> \\t \\t',loader.source.vocab.stoi['<sos>'])\n",
    "print('two \\t \\t',loader.source.vocab.stoi['two'])\n",
    "print('young \\t \\t',loader.source.vocab.stoi['young'])\n",
    "print(', \\t \\t',loader.source.vocab.stoi[','])\n",
    "print('<eos> \\t \\t',loader.source.vocab.stoi['<eos>'])\n",
    "print('<pad> \\t \\t',loader.source.vocab.stoi['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8425a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, valid_iter, test_iter = loader.make_iter(train, valid, test,\n",
    "                                                     batch_size=batch_size,\n",
    "                                                     device=device)\n",
    "print('----3. 从迭代器中取一对，可见其开头为<sos>2, 结尾<eos>3， 剩余为<pad>1---------------')\n",
    "print('padding的作用：一个batch中有不同的句子， 句子里最大句长为l, 小于l的句子都填充<pad>1')\n",
    "for batch in train_iter:\n",
    "    print(batch.src[0])\n",
    "    print(batch.trg[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b221ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----4. 以下词表参数也是模型中重要的部分----')\n",
    "src_pad_idx = loader.source.vocab.stoi['<pad>']\n",
    "trg_pad_idx = loader.target.vocab.stoi['<pad>']\n",
    "trg_sos_idx = loader.target.vocab.stoi['<sos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b522b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_voc_size = len(loader.source.vocab)\n",
    "print(\"嵌入层的输入参数 {} x 维度 {}\".format(enc_voc_size,d_model))\n",
    "dec_voc_size = len(loader.target.vocab)\n",
    "print(\"全链接层输出维度 {} x 输出词表{}：\".format(d_model,dec_voc_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b353a940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从data中获取数据\n",
    "# 仅运行一次，保证测试时使用同一组数据\n",
    "\n",
    "for i, batch in enumerate(train_iter):\n",
    "    src = batch.src\n",
    "    trg = batch.trg\n",
    "    print(\"save src shape:\",src.shape)\n",
    "    print(\"save trg shape\",trg.shape)\n",
    "    torch.save(src, 'tensor_src.pt')\n",
    "    torch.save(trg, 'tensor_trg.pt')\n",
    "    break\n",
    "\n",
    "test_src = torch.load('tensor_src.pt')\n",
    "test_trg = torch.load('tensor_trg.pt')\n",
    "print(\"load src shape\", test_src.shape)\n",
    "print(\"load trg shape\", test_trg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cac8423",
   "metadata": {},
   "source": [
    "## 1.5 评价指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8e7c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def calculate_bleu(reference, candidate):\n",
    "    reference = [reference.split()]\n",
    "    candidate = candidate.split()\n",
    "    smoothing_function = nltk.translate.bleu_score.SmoothingFunction()\n",
    "    bleu_score = nltk.translate.bleu_score.sentence_bleu(reference, candidate, smoothing_function=smoothing_function.method1)\n",
    "    return bleu_score\n",
    "\n",
    "# 示例用法\n",
    "reference_sentence = \"The cat is on the mat\"\n",
    "# candidate_sentence = \"The cat is sitting on the mat\"\n",
    "candidate_sentence = \"The cat is on the mat\"\n",
    "bleu = calculate_bleu(reference_sentence, candidate_sentence)\n",
    "print(\"BLEU score:\", bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d4bbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "def calculate_rouge(reference, candidate):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(candidate, reference)\n",
    "    rouge_1 = scores[0]['rouge-1']['f']\n",
    "    rouge_2 = scores[0]['rouge-2']['f']\n",
    "    rouge_l = scores[0]['rouge-l']['f']\n",
    "    return rouge_1, rouge_2, rouge_l\n",
    "\n",
    "# 示例用法\n",
    "reference_summary = \"The cat is on the mat\"\n",
    "candidate_summary = \"The cat is sitting on the mat\"\n",
    "rouge_1, rouge_2, rouge_l = calculate_rouge(reference_summary, candidate_summary)\n",
    "print(\"ROUGE-1 score:\", rouge_1)\n",
    "print(\"ROUGE-2 score:\", rouge_2)\n",
    "print(\"ROUGE-L score:\", rouge_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b99a07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiwer\n",
    "\n",
    "def calculate_wer(reference, candidate):\n",
    "    wer = jiwer.wer(reference, candidate)\n",
    "    return wer\n",
    "\n",
    "# 示例用法\n",
    "reference_transcription = \"The cat is on the mat\"\n",
    "candidate_transcription = \"The cat is sitting on the mat\"\n",
    "wer = calculate_wer(reference_transcription, candidate_transcription)\n",
    "print(\"WER score:\", wer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ad6f5e",
   "metadata": {},
   "source": [
    "## 2. 手撕Transformer模型\n",
    "\n",
    "这个章节主要理解模型构造的过程，第3章会自顶向下debug 数据流"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac40f53e",
   "metadata": {},
   "source": [
    "### 2.1.1 Token Embedding\n",
    "目的将1个token转成一串向量\n",
    "参照Word2Vec算法原理如下图示\n",
    "\n",
    "Embdding Vec\n",
    "数据类型流向 word(string) -> 【token(int) -> vec(list(float))】\n",
    "\n",
    "以下为两个词对应的vec进行比较， 越相近的向量，词性相同\n",
    "![0](image/embeddings-cosine-personality.png)\n",
    "\n",
    "Word2Vec embedding\n",
    "\n",
    "纵轴词表数量， 横轴vec词向量维度， 期望找出当前单词和右边相近的单词向量\n",
    "\n",
    "\n",
    "![1](image/word2vec-lookup-embeddings.png)\n",
    "\n",
    "SkipGram: \n",
    "\n",
    "假设\"我是小冬瓜\", 对于\"冬\"单词与\"小\"和\"瓜\"相近positive，与\"我\"间隔较远\n",
    "![2](image/skipgram-sliding-window-5.png)\n",
    "\n",
    "Data and model\n",
    "\n",
    "则对于\"冬\"则与\"冬-小\"和\"冬-瓜\"相近label则为1， 人为构造负样本\"冬-控\",\"冬-龙\",\"冬-抗\",\"冬-狼\"设置label为0\n",
    "![3](image/word2vec-training-example-2.png)\n",
    "\n",
    "根据所构造的样本，即可训练词表\n",
    "\n",
    "Train error\n",
    "![4](image/word2vec-training-update.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958e97e6",
   "metadata": {},
   "source": [
    "## embedding 实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3b09af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "embd_layer = torch.nn.Embedding(14, 512)\n",
    "print('embedding.weight', embd_layer.weight.shape)\n",
    "\n",
    "print(embd_layer.weight[4][:10])\n",
    "\n",
    "input_id = torch.tensor([[2, 4, 5, 6, 7, 8, 3, 1, 1, 1], \n",
    "                      [2, 4, 9, 10,11,12,13,3, 1, 1],\n",
    "                      [2, 6, 7, 8, 9, 10,11,12,13,3]])\n",
    "\n",
    "\n",
    "print(\"输入数据\",input_id.shape)\n",
    "print(\"输入数据的embedding\", embd_layer(input_id).shape)\n",
    "\n",
    "print(embd_layer(input_id)[0][1][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c58949",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"embedding更多直接了解word2vec:\")\n",
    "print(\"按照以上理论可以直接，通过torch创建embedding表\")\n",
    "a = nn.Embedding(enc_voc_size, d_model)\n",
    "# embedding_layer = nn.Embedding(14, 128)\n",
    "print(a.weight.shape) # 14 * 128\n",
    "print(input_id.shape) # \n",
    "x = a(input_id)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87437afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建Token embedding类\n",
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super(TokenEmbedding, self).__init__(vocab_size, d_model, padding_idx=1)\n",
    "        \n",
    "test_src_token = TokenEmbedding(enc_voc_size, d_model) #对 src：en 进行embedding\n",
    "test_trg_token = TokenEmbedding(dec_voc_size, d_model) #对 trg：de 进行embedding\n",
    "print(test_src_token) \n",
    "print(test_trg_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd393f6",
   "metadata": {},
   "source": [
    "### 2.1.2 position encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4576a629",
   "metadata": {},
   "source": [
    "Position 编码公式\n",
    "\n",
    "十进制13  ->  二进制(1,1,0,1) 这是一种位置编码向量: transformer中则使用连续函数描述向量的生成。\n",
    "\n",
    "可直接记住公式， 也可以尝试通俗理解以下过程\n",
    "\n",
    "(1,1,0,1)  两两成组 (1,1) (0,1) -> 4维/2=2组： 两组index为 i+1, i \n",
    "\n",
    "position encoding后为： (sin(13/(i+1)),cos/(13(i+1))、 ((sin(13/i),cos(13/i)))\n",
    "\n",
    "则最后 (1,1,0,1) ->  (sin(13/(i+1)),cos(13(i+1))、 ((sin(13/i),cos(13/i)))\n",
    "\n",
    "\n",
    "![title](image/positional_encoding.jpg)\n",
    "\n",
    "\n",
    "以下为一种可视化理解如何从p,i变量生成位置编码\n",
    "\n",
    "![pos](image/position_embeding_pos.png)\n",
    "![pos_i](image/Fhc4M.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4d4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len, device):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model, device=device)\n",
    "        self.encoding.requires_grad = False  \n",
    "        pos = torch.arange(0, max_len, device=device)\n",
    "        pos = pos.float().unsqueeze(dim=1)\n",
    "        _2i = torch.arange(0, d_model, step=2, device=device).float()\n",
    "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
    "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
    "        # 512\n",
    "        # 2x256 cos sin\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        return self.encoding[:seq_len, :]\n",
    "\n",
    "test_pos_encoding = PositionalEncoding(d_model, max_len, device)\n",
    "print(test_pos_encoding.encoding.shape)\n",
    "print(test_pos_encoding.encoding[255,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb372ef9",
   "metadata": {},
   "source": [
    "### 2.1.3 LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83ee253",
   "metadata": {},
   "source": [
    "layer norm 公式\n",
    "\n",
    "原图公式与主要四行代码一一对应\n",
    "\n",
    "layernorm作用在最后一维进行归一化\n",
    "\n",
    "![layer](image/layer_norm.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da48a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-12):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # layernorm作用在(-1) 最后一维进行归一化\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        var = x.var(-1, unbiased=False, keepdim=True)\n",
    "        out = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        out = self.gamma * out + self.beta\n",
    "        return out\n",
    "    \n",
    "test_ln = LayerNorm(d_model)\n",
    "print(test_ln.gamma.shape)\n",
    "print(test_ln.beta.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dad0e64",
   "metadata": {},
   "source": [
    "### 2.1.4 Scaled-Dot-Production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbbcd37",
   "metadata": {},
   "source": [
    "scaled dot product 图示\n",
    "class ScaleDotProductAttention(nn.Module)\n",
    "![attention](image/scale_dot_product_attention.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bee67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单头注意力机制\n",
    "# 图-代码-公式完全对应， 第3章节有详细推导\n",
    "# 先记住实现\n",
    "class ScaleDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaleDotProductAttention, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None, e=1e-12):\n",
    "        batch_size, head, length, d_tensor = k.size() # /n_embd/8\n",
    "        k_t = k.transpose(2, 3) \n",
    "        score = (q @ k_t) / math.sqrt(d_tensor) #qk^t/dk\n",
    "        if mask is not None:\n",
    "            score = score.masked_fill(mask == 0, -10000)\n",
    "        score = self.softmax(score) #softmax(qk^t/dk)\n",
    "        v = score @ v #softmax(qk^t/dk)*V\n",
    "        return v, score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db49dfb4",
   "metadata": {},
   "source": [
    "### 2.2.1 position wise feed forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1366cc6c",
   "metadata": {},
   "source": [
    "ffn\n",
    "![layer](image/positionwise_feed_forward.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41450fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前向传播，当成神经网络全链接层 + 隐含层理解\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "ffw = PositionwiseFeedForward(d_model, ffn_hidden)\n",
    "print(ffw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46237acc",
   "metadata": {},
   "source": [
    "### 2.2.2 Multi-Head-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95acfcb8",
   "metadata": {},
   "source": [
    "multi-head-attention\n",
    "![multiheadattention](image/multi_head_attention.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc59d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    # dmodel_n_embed; 512 8\n",
    "    def __init__(self, d_model, n_head):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.attention = ScaleDotProductAttention()\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_concat = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)  # 对应图里liner：先对QKV投影\n",
    "        q, k, v = self.split(q), self.split(k), self.split(v) # Q->Q0, Q1, ... \n",
    "        out, attention = self.attention(q, k, v, mask=mask) # 每一头计算attention，z0, z1, ...\n",
    "        out = self.concat(out) # 将每一头拼接 z0 z1 .. = z\n",
    "        out = self.w_concat(out) # z -> linner -> output\n",
    "        return out\n",
    "\n",
    "    # 先不用看实现，后面会讲\n",
    "    def split(self, tensor):\n",
    "        batch_size, length, d_model = tensor.size()\n",
    "        d_tensor = d_model // self.n_head\n",
    "        tensor = tensor.view(batch_size, length, self.n_head, d_tensor).transpose(1, 2)\n",
    "        return tensor\n",
    "\n",
    "    # 先不用看实现，后面会讲\n",
    "    def concat(self, tensor):\n",
    "        batch_size, head, length, d_tensor = tensor.size()\n",
    "        d_model = head * d_tensor\n",
    "        tensor = tensor.transpose(1, 2).contiguous().view(batch_size, length, d_model)\n",
    "        return tensor\n",
    "    \n",
    "test_multihead_attention = MultiHeadAttention(d_model, n_heads)\n",
    "print(test_multihead_attention)\n",
    "print(d_model, n_heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a144278",
   "metadata": {},
   "source": [
    "### 2.2.3 Transformer Embeding\n",
    "\n",
    "model.png：见input后的操作符token+position\n",
    "\n",
    "![model](image/model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe6f68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer—embedding数据流：【嵌入向量+位置编码 ->  X】 -> QKV -> X\n",
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len, drop_prob, device):\n",
    "        super(TransformerEmbedding, self).__init__()\n",
    "        self.tok_emb = TokenEmbedding(vocab_size, d_model)\n",
    "        self.pos_emb = PositionalEncoding(d_model, max_len, device)\n",
    "        self.drop_out = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tok_emb = self.tok_emb(x)\n",
    "        pos_emb = self.pos_emb(x)\n",
    "        # 记住这里还有个Dropout\n",
    "        return self.drop_out(tok_emb + pos_emb)\n",
    "    \n",
    "test_embedding = TransformerEmbedding(enc_voc_size, d_model, max_len, drop_prob, device)\n",
    "print(test_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66825129",
   "metadata": {},
   "source": [
    "### 2.3.1 Transformer Encode Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab7d373",
   "metadata": {},
   "source": [
    "编解码：enc-dec\n",
    "\n",
    "特别注意【每个 decoder block】都需要接受encoder的输出\n",
    "\n",
    "![enc-dec](image/enc_dec.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90e9221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单独一个encoder block\n",
    "# 多个 encoder block 组成一个 encoder\n",
    "\n",
    "# 可以叫encoder-layer 也可以叫 encoder-block\n",
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
    "        self.norm1 = LayerNorm(d_model=d_model)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm2 = LayerNorm(d_model=d_model)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x, s_mask):\n",
    "        # 1. compute self attention\n",
    "        # print(\"encoder layer x: \", x.shape)\n",
    "        _x = x\n",
    "        x = self.attention(q=x, k=x, v=x, mask=s_mask)\n",
    "        \n",
    "        # 2. add and norm\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + _x)\n",
    "        \n",
    "        # 3. positionwise feed forward network\n",
    "        _x = x\n",
    "        x = self.ffn(x)\n",
    "      \n",
    "        # 4. add and norm\n",
    "        x = self.dropout2(x)\n",
    "        x = self.norm2(x + _x)\n",
    "        return x\n",
    "test_encoder_block = EncoderLayer(d_model, ffn_hidden, n_heads, drop_prob)\n",
    "print(test_encoder_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee7c12b",
   "metadata": {},
   "source": [
    "### 2.3.2 Transformer Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4e023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
    "        self.norm1 = LayerNorm(d_model=d_model)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        # enc_dec_attention使用encoder的 Q， decoder的 K，V\n",
    "        self.enc_dec_attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
    "        self.norm2 = LayerNorm(d_model=d_model)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm3 = LayerNorm(d_model=d_model)\n",
    "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, dec, enc, t_mask, s_mask):\n",
    "        # 1. compute self attention\n",
    "        _x = dec\n",
    "        x = self.self_attention(q=dec, k=dec, v=dec, mask=t_mask)#下三角矩阵\n",
    "        \n",
    "        # 2. add and norm\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + _x)\n",
    "\n",
    "        if enc is not None:\n",
    "            # 3. compute encoder - decoder attention\n",
    "            _x = x\n",
    "            x = self.enc_dec_attention(q=x, k=enc, v=enc, mask=s_mask) # \n",
    "            \n",
    "            # 4. add and norm\n",
    "            x = self.dropout2(x)\n",
    "            x = self.norm2(x + _x)\n",
    "\n",
    "        # 5. positionwise feed forward network\n",
    "        _x = x\n",
    "        x = self.ffn(x)\n",
    "        \n",
    "        # 6. add and norm\n",
    "        x = self.dropout3(x)\n",
    "        x = self.norm3(x + _x)\n",
    "        return x\n",
    "test_decoder_block = DecoderLayer(d_model, ffn_hidden, n_heads, drop_prob)\n",
    "print(test_decoder_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814af91f",
   "metadata": {},
   "source": [
    "### 2.3.3 Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d14b6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, enc_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device):\n",
    "        super().__init__()\n",
    "        self.emb = TransformerEmbedding(d_model=d_model,\n",
    "                                        max_len=max_len,\n",
    "                                        vocab_size=enc_voc_size,\n",
    "                                        drop_prob=drop_prob,\n",
    "                                        device=device)\n",
    "\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model=d_model,\n",
    "                                                  ffn_hidden=ffn_hidden,\n",
    "                                                  n_head=n_head,\n",
    "                                                  drop_prob=drop_prob)\n",
    "                                     for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x, s_mask):\n",
    "        x = self.emb(x)\n",
    "        # 每个encoder block的输入输出tensor是一致的\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, s_mask)\n",
    "        return x\n",
    "    \n",
    "\n",
    "test_encoder = Encoder(enc_voc_size, max_len, d_model, ffn_hidden, n_heads, n_layers, drop_prob, device)\n",
    "print(\"encoder block size : \", len(test_encoder.layers))\n",
    "print(test_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc0cbb0",
   "metadata": {},
   "source": [
    "### 2.3.4 Transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecf1d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dec_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device):\n",
    "        super().__init__()\n",
    "        self.emb = TransformerEmbedding(d_model=d_model,\n",
    "                                        drop_prob=drop_prob,\n",
    "                                        max_len=max_len,\n",
    "                                        vocab_size=dec_voc_size,\n",
    "                                        device=device)\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model=d_model,\n",
    "                                                  ffn_hidden=ffn_hidden,\n",
    "                                                  n_head=n_head,\n",
    "                                                  drop_prob=drop_prob)\n",
    "                                     for _ in range(n_layers)])\n",
    "\n",
    "        self.linear = nn.Linear(d_model, dec_voc_size)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        trg = self.emb(trg)\n",
    "\n",
    "        # 这里的每个layer，都有decoder的enc_src输入\n",
    "        for layer in self.layers:\n",
    "            trg = layer(trg, enc_src, trg_mask, src_mask) # src_trg_mask\n",
    "\n",
    "        # pass to LM head\n",
    "        output = self.linear(trg)\n",
    "        return output\n",
    "\n",
    "test_decoder = Decoder(dec_voc_size, max_len, d_model, ffn_hidden, n_heads, n_layers, drop_prob, device)\n",
    "print(\"decoder block size : \", len(test_decoder.layers))\n",
    "print(test_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bcdea6",
   "metadata": {},
   "source": [
    "### 2.4 Transformer结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5397e67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完整的Transfomer 类， 创建encoder / decoder\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, src_pad_idx, trg_pad_idx, trg_sos_idx, enc_voc_size, dec_voc_size, d_model, n_head, max_len,\n",
    "                 ffn_hidden, n_layers, drop_prob, device):\n",
    "        super().__init__()\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.trg_sos_idx = trg_sos_idx\n",
    "        self.device = device\n",
    "        self.encoder = Encoder(d_model=d_model,\n",
    "                               n_head=n_head,\n",
    "                               max_len=max_len,\n",
    "                               ffn_hidden=ffn_hidden,\n",
    "                               enc_voc_size=enc_voc_size,\n",
    "                               drop_prob=drop_prob,\n",
    "                               n_layers=n_layers,\n",
    "                               device=device)\n",
    "\n",
    "        self.decoder = Decoder(d_model=d_model,\n",
    "                               n_head=n_head,\n",
    "                               max_len=max_len,\n",
    "                               ffn_hidden=ffn_hidden,\n",
    "                               dec_voc_size=dec_voc_size,\n",
    "                               drop_prob=drop_prob,\n",
    "                               n_layers=n_layers,\n",
    "                               device=device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_pad_mask(src, src, self.src_pad_idx, self.src_pad_idx)\n",
    "\n",
    "        src_trg_mask = self.make_pad_mask(trg, src, self.trg_pad_idx, self.src_pad_idx)\n",
    "\n",
    "        trg_mask = self.make_pad_mask(trg, trg, self.trg_pad_idx, self.trg_pad_idx) * \\\n",
    "                   self.make_no_peak_mask(trg, trg)\n",
    "        # encoder计算流程 src -> encoder -> enc_src\n",
    "        # decoder计算流程 enc_src + trg -> decoder  -> output\n",
    "        # 关于Mask后面会讲解\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        output = self.decoder(trg, enc_src, trg_mask, src_trg_mask)\n",
    "        return output\n",
    "\n",
    "    def make_pad_mask(self, q, k, q_pad_idx, k_pad_idx):\n",
    "        len_q, len_k = q.size(1), k.size(1)\n",
    "\n",
    "        # batch_size x 1 x 1 x len_k\n",
    "        k = k.ne(k_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # batch_size x 1 x len_q x len_k\n",
    "        k = k.repeat(1, 1, len_q, 1)\n",
    "\n",
    "        # batch_size x 1 x len_q x 1\n",
    "        q = q.ne(q_pad_idx).unsqueeze(1).unsqueeze(3)\n",
    "        # batch_size x 1 x len_q x len_k\n",
    "        q = q.repeat(1, 1, 1, len_k)\n",
    "\n",
    "        mask = k & q\n",
    "        return mask\n",
    "\n",
    "    def make_no_peak_mask(self, q, k):\n",
    "        len_q, len_k = q.size(1), k.size(1)\n",
    "        # len_q x len_k\n",
    "        mask = torch.tril(torch.ones(len_q, len_k)).type(torch.BoolTensor).to(self.device)\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c6828f",
   "metadata": {},
   "source": [
    "## 3. 调试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d94ccb1",
   "metadata": {},
   "source": [
    "### 3.1 创建Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723837b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transformer为./models/transformer.py里的模型类，包含多个对象和方法\n",
    "\n",
    "model = Transformer(src_pad_idx=src_pad_idx,\n",
    "                    trg_pad_idx=trg_pad_idx,\n",
    "                    trg_sos_idx=trg_sos_idx,\n",
    "                    d_model=d_model,\n",
    "                    enc_voc_size=enc_voc_size,\n",
    "                    dec_voc_size=dec_voc_size,\n",
    "                    max_len=max_len,\n",
    "                    ffn_hidden=ffn_hidden,\n",
    "                    n_head=n_heads,\n",
    "                    n_layers=n_layers,\n",
    "                    drop_prob=drop_prob,\n",
    "                    device=device).to(device)\n",
    "\n",
    "# 使用kaiming_uniform对model初始化\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.kaiming_uniform(m.weight.data)\n",
    "        \n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc37af7b",
   "metadata": {},
   "source": [
    "### 3.2 创建调试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a769fcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 从data中获取数据\n",
    "# for i, batch in enumerate(train_iter):\n",
    "#     src = batch.src\n",
    "#     trg = batch.trg\n",
    "#     print(\"save src shape:\",src.shape)\n",
    "#     print(\"save trg shape\",trg.shape)\n",
    "#     torch.save(src, 'tensor_src.pt')\n",
    "#     torch.save(trg, 'tensor_trg.pt')\n",
    "#     break\n",
    "\n",
    "test_src = torch.load('tensor_src.pt')\n",
    "test_trg = torch.load('tensor_trg.pt')\n",
    "print(\"load src shape\", test_src.shape)\n",
    "print(\"load trg shape\", test_trg.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a34ec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集, 从dataloader中获取\n",
    "# 接下来所有数据计算，都基于batch(128)\n",
    "\n",
    "src = torch.load('tensor_src.pt')\n",
    "trg = torch.load('tensor_trg.pt')\n",
    "print(\"load src shape\", src.shape)\n",
    "print(\"load trg shape\", trg.shape)\n",
    "print('batch size : {} and src length: {} '.format(src.shape[0], src.shape[1]))\n",
    "print('batch size : {} and trg length: {} '.format(trg.shape[0], trg.shape[1]))\n",
    "print('src [0]: ', src[0])\n",
    "print('trg [0]: ', trg[0])\n",
    "print('src_pad_idx:',src_pad_idx)\n",
    "print('trg_pad_idx:',trg_pad_idx)\n",
    "print('trg_sos_idx:',trg_sos_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a131f78",
   "metadata": {},
   "source": [
    "### 3.3 创建mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be21417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据pad信息，创建mask，先忽略实现细节\n",
    "src_mask = model.make_pad_mask(src, src, src_pad_idx, src_pad_idx)\n",
    "src_trg_mask = model.make_pad_mask(trg, src, trg_pad_idx, src_pad_idx)\n",
    "trg_mask = model.make_pad_mask(trg, trg, trg_pad_idx, trg_pad_idx) * \\\n",
    "            model.make_no_peak_mask(trg, trg)\n",
    "print(\"src_mask:\", src_mask.shape)\n",
    "print(\"src_trg_mask:\", src_trg_mask.shape)\n",
    "print(\"trg_mask:\", trg_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28a7e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(src_mask[0][0].int())\n",
    "# print(src_trg_mask[0][0].int())\n",
    "# # trg.Q.shape() * src.K^T.shape()\n",
    "# print(trg_mask[0][0].int()) # 下三角"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8575e6",
   "metadata": {},
   "source": [
    "### 3.4 图解Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa48f39",
   "metadata": {},
   "source": [
    "![all](image/the_transformer_3.png)\n",
    "![all](image/The_transformer_encoders_decoders.png)\n",
    "![all](image/The_transformer_encoder_decoder_stack.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be24580",
   "metadata": {},
   "source": [
    "### 3.4.1 计算src->[encoder->decoder]->target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e8f722",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419f50d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # transformer 编码层和解码层计算\n",
    "# print(\"查看模型：\", model)\n",
    "enc_src = model.encoder(src, src_mask)\n",
    "output = model.decoder(trg, enc_src, trg_mask, src_trg_mask)\n",
    "print(src.shape)\n",
    "print(enc_src.shape)\n",
    "print(output.shape)\n",
    "print(\"decode voc size:\", dec_voc_size)\n",
    "print(\"d_model:\", d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e193a7",
   "metadata": {},
   "source": [
    "![embedding](image/transformer_positional_encoding_vectors.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1ae0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder 编码层计算\n",
    "# encoder包含emb和n_layers层\n",
    "\n",
    "emb_src = model.encoder.emb(src)\n",
    "print('src:', emb_src.shape)\n",
    "print('emb_src:', emb_src.shape)\n",
    "print('n_layers:', n_layers)\n",
    "print('encode layers:', len(model.encoder.layers))\n",
    "# encoder0 -> encoder1\n",
    "for layer in model.encoder.layers:\n",
    "    encoder_src = layer(emb_src, src_mask)\n",
    "    print('encoder_src:', encoder_src.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0231654",
   "metadata": {},
   "source": [
    "### 3.4.2 计算input->embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44d52b7",
   "metadata": {},
   "source": [
    "数值position\n",
    "![embedding-sample](image/transformer_positional_encoding_example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9406dae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding 嵌入层计算\n",
    "# models/embedding/transformer_embedding.py\n",
    "# class TransformerEmbedding(nn.Module)\n",
    "\n",
    "emb = model.encoder.emb\n",
    "print(emb)\n",
    "tok_emb = emb.tok_emb(src)\n",
    "pos_emb = emb.pos_emb(src)\n",
    "emb_out = emb.drop_out(tok_emb + pos_emb)\n",
    "print('src:', src.shape)\n",
    "print('tok_emb:', tok_emb.shape)\n",
    "print('pos_emb:', pos_emb.shape)\n",
    "print('emb_out:', emb_out.shape)\n",
    "\n",
    "# tok_emb 使用 nn.embedding\n",
    "# pos_emb 计算如下\n",
    "# 512 / 2[cos/sin] -> i 256\n",
    "print('\\n-----------------------手撕position编码-----------------------')\n",
    "# 位置编码仅计算一次\n",
    "emb.pos_emb.encoding = torch.zeros(max_len, d_model)\n",
    "print(\"位置编码向量tensor: \",emb.pos_emb.encoding.shape)\n",
    "\n",
    "emb.pos_emb.encoding.requires_grad = False  # we don't need to compute gradient\n",
    "pos = torch.arange(0, max_len)\n",
    "print('pos:', pos.shape)\n",
    "pos = pos.float().unsqueeze(dim=1)\n",
    "print('pos 增加一个维度后:', pos.shape)\n",
    "\n",
    "_2i = torch.arange(0, d_model, step=2, device=device).float()\n",
    "print('_2i ', _2i.shape)\n",
    "print('_2i[0:10] ', _2i[:10])\n",
    "\n",
    "print('赋值pos_embeding')\n",
    "emb.pos_emb.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
    "emb.pos_emb.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
    "print('--------------:', emb.pos_emb.encoding.shape)\n",
    "print(\"打印前10个数据\", emb.pos_emb.encoding[0:5, 0:5])\n",
    "\n",
    "# 使用时\n",
    "batch_size, seq_len = src.size()\n",
    "print(batch_size)\n",
    "print(seq_len)\n",
    "print(emb.pos_emb.encoding[:seq_len, :].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbcc170",
   "metadata": {},
   "source": [
    "### 3.4.3 计算 embedding->[encoder block]->output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3408496c",
   "metadata": {},
   "source": [
    "encoder block 主要包含multi-head attention 和 feed forward position两个主要模块\n",
    "\n",
    "![encoder_block](image/Transformer_encoder.png)\n",
    "\n",
    "----\n",
    "\n",
    "encoder block更加具体为\n",
    "\n",
    "![encoder_block detail](image/transformer_resideual_layer_norm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f35d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 请尝试独自debug各层类\n",
    "\n",
    "# 获取encode中的一个blocks\n",
    "layer=model.encoder.layers[0]\n",
    "# print(layer)\n",
    "\n",
    "# 0. 保留输入向量, 用于short cut\n",
    "emb_src = emb_out\n",
    "_emb_src = emb_src \n",
    "\n",
    "# 1. 编码层 多头-自注意力机制（后面会详细介绍）\n",
    "x = layer.attention(q=emb_src, k=emb_src, v=emb_src, mask=src_mask)\n",
    "\n",
    "# 2. dropout和layer-norm（后面会介绍）\n",
    "x = layer.dropout1(x)\n",
    "x = layer.norm1(x + _emb_src) # shorcut连接\n",
    "\n",
    "# 3. 基于位置的前向传播将维度512->2048->512\n",
    "_x = x\n",
    "x = layer.ffn(x)\n",
    "\n",
    "# 4. dropout + shortcut + layer-norm\n",
    "x = layer.dropout2(x)\n",
    "x = layer.norm2(x + _x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb119211",
   "metadata": {},
   "source": [
    "shortcut目的在以保留信息，防止信息损失，见ResNet\n",
    "\n",
    "![shortcut](image/transformer_resideual_layer_norm_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae7a56d",
   "metadata": {},
   "source": [
    "### 3.4.4 计算 embeding->[multi-head-attention]->score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a90d2b7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "输入输出\n",
    "![multi-head](image/transformer_attention_heads_z.png)\n",
    "\n",
    "\n",
    "------\n",
    "输出拼接\n",
    "![multi-concate](image/transformer_attention_heads_weight_matrix_o.png)\n",
    "\n",
    "-----\n",
    "Multi-head-attention计算流程\n",
    "![multi-head-attention-pipeline](image/transformer_multi-headed_self-attention-recap.png)\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "![wq](image/self-attention-matrix-calculation.png)\n",
    "---\n",
    "\n",
    "\n",
    "![8头](image/transformer_attention_heads_qkv.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1a487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multihead多头注意力计算\n",
    "\n",
    "# encode multi-attention直接计算多头注意力分数\n",
    "multi_head_attention = model.encoder.layers[0].attention\n",
    "print(\"multi_head_attention层包含:\", multi_head_attention)\n",
    "x_attention_out = multi_head_attention(q=emb_src, k=emb_src, v=emb_src, mask=src_mask)\n",
    "print(\"emb_src:\", emb_src.shape)\n",
    "print(\"x_attention_out:\", x_attention_out.shape)\n",
    "print(\"以下为多头注意力forward分解步骤：\")\n",
    "\n",
    "# 0. 自注意力向量\n",
    "q = k = v = emb_src\n",
    "print(\"\\n 0. 输入向量emb_src:\", emb_src.shape)\n",
    "print(\"q.shape:\", q.shape)\n",
    "print(\"k.shape:\", k.shape)\n",
    "print(\"v.shape:\", v.shape)\n",
    "\n",
    "# 1. liner转化\n",
    "print(\"\\n 1. 对qkv liner 转化\")\n",
    "q = multi_head_attention.w_q(q)\n",
    "k = multi_head_attention.w_k(k)\n",
    "v = multi_head_attention.w_v(v)\n",
    "print(\"q=f(q): \", q.shape)\n",
    "\n",
    "# 2. 将输入向量拆成n_head\n",
    "print(\"\\n 2. 将输入向量拆成n_head\")\n",
    "print(\"n_heads:\", n_heads)\n",
    "print(\"multi_head_attention.n_head:\", multi_head_attention.n_head)\n",
    "_q = q\n",
    "\n",
    "# do split multi_head_attention.split()\n",
    "print('*-------multi_head_attention.split()-------------*')\n",
    "batch_size, length, d_model = _q.size()\n",
    "d_tensor = d_model // multi_head_attention.n_head\n",
    "print(\"d_model:{} / n_heads:{} = d_tensor:{}\".format(d_model, n_heads, d_tensor))\n",
    "print(\"单头向量维度为:\", d_tensor)\n",
    "_q_split = _q.view(batch_size, length, multi_head_attention.n_head, d_tensor).transpose(1, 2)\n",
    "print(\"_q_split:\", _q_split.shape)\n",
    "print('*-------multi_head_attention.split()-------------*')\n",
    "\n",
    "q, k, v = multi_head_attention.split(q), multi_head_attention.split(k), multi_head_attention.split(v)\n",
    "print(\"shape = [batch_size:128, heads:8, length:29, d_tensor:64]\")\n",
    "print(\"multi_head_attention.split(q):\", q.shape)\n",
    "print(\"multi_head_attention.split(k):\", k.shape)\n",
    "print(\"multi_head_attention.split(v):\", v.shape)\n",
    "\n",
    "\n",
    "# 3. do scale dot product to compute similarity\n",
    "# 计算每一头的attention（scale and dot attention）\n",
    "print(\"\\n 3. 计算单头注意力, scale and dot attention\")\n",
    "print(\"上面将512维度分成8头64维\")\n",
    "print(\"会独立介绍单头注意力的计算\")\n",
    "_q_single = q\n",
    "_k_single = k\n",
    "_v_single = v\n",
    "out, attention = multi_head_attention.attention(q, k, v, mask=src_mask)\n",
    "print(\"对每一头进行自注意力后的结果:\", out.shape)\n",
    "\n",
    "# 4. concat and pass to linear layer\n",
    "print(\"\\n 4. 将8头64维拼接成512维度向量\")\n",
    "_out = out \n",
    "# do concat \n",
    "\n",
    "print('*-------multi_head_attention.concat()-------------*')\n",
    "print(\"multi_head_attention.concat() 函数示例\")\n",
    "batch_size, head, length, d_tensor = _out.size()\n",
    "d_model = head * d_tensor\n",
    "_out_concat = _out.transpose(1, 2).contiguous().view(batch_size, length, d_model)\n",
    "print(\"concat 操作后\", _out_concat.shape)\n",
    "print('*-------multi_head_attention.concat()-------------*')\n",
    "\n",
    "out = multi_head_attention.concat(out)\n",
    "print(\"after concat out shape:\", out.shape)\n",
    "out = multi_head_attention.w_concat(out)\n",
    "print(\"对多头注意力输出再进行前向传播\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3157140",
   "metadata": {},
   "source": [
    "### 3.4.5 计算 [scale-dot-production] :  mask(q@k^t/scaled)@v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d63c6d",
   "metadata": {},
   "source": [
    "![pipeline](image/self-attention-matrix-calculation-2.png)\n",
    "![pipeline_qkv2](image/self-attention-output.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448c76aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention, 单头注意力计算\n",
    "# models/layer/scale_dot_product_attention.py\n",
    "# class ScaleDotProductAttention(nn.Module)\n",
    "\n",
    "attention = multi_head_attention.attention\n",
    "print(attention)\n",
    "\n",
    "# input is 4 dimension tensor\n",
    "# [batch_size, head, length, d_tensor]\n",
    "k = _k_single\n",
    "q = _q_single\n",
    "v = _v_single\n",
    "batch_size, head, length, d_tensor = k.size()\n",
    "\n",
    "print('tensor中的格式： 只关注length句长， d_tensor向量长度')\n",
    "print('[batch_size:{}, head:{}, length:{}, d_tensor:{}]'.format(batch_size,head,length,d_tensor))\n",
    "\n",
    "# 1. dot product Query with Key^T to compute similarity\n",
    "k_t = k.transpose(2, 3)  # transpose\n",
    "\n",
    "print(\"q:\", q.shape)\n",
    "print(\"k_t:\", k_t.shape)\n",
    "score = (q @ k_t) / math.sqrt(d_tensor)  # scaled dot product\n",
    "########   dot   ####### scaled #######\n",
    "print(\"通过计算两个向量的点积dot操作:score=q@k_t: \", score.shape)\n",
    "print(\"每个词与词之间计算相关性\")\n",
    "print(\"score代表注意力分数 \")\n",
    "print(\"src_mask:\", src_mask.shape)\n",
    "# 2. apply masking (opt)\n",
    "if src_mask is not None:\n",
    "    score = score.masked_fill(src_mask == 0, -10000)\n",
    "# 3. pass them softmax to make [0, 1] range\n",
    "score = attention.softmax(score)\n",
    "# 4. multiply with Value\n",
    "print(\"v:\", v.shape)\n",
    "v = score @ v\n",
    "print(\"score * v:\", v.shape)\n",
    "print(\"score * v: 代表注意力特征向量，即每个词在当前这个句子中的特征表达\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29fd07f",
   "metadata": {},
   "source": [
    "\n",
    "一个句子中：关于'it'单词的 单头自注意力score 30个词 [1,30,1] 'it'\n",
    "![vis-1](image/transformer_self-attention_visualization.png)\n",
    "\n",
    "一个句子中：关于'it'单词的 两头自注意力score 30个词 [2, 30,1] 'it'\n",
    "![vis-2](image/transformer_self-attention_visualization_2.png)\n",
    "\n",
    "\n",
    "一个句子中：关于'it'单词的 八头自注意力score 30个词 [8, 30,1] 'it'\n",
    "![vis-3](image/transformer_self-attention_visualization_3.png)\n",
    "\n",
    "\n",
    "一个句子中：关于30个单词的 八头自注意力score 30个词 [8, 30,30] \n",
    "\n",
    "128个句子中：关于30个单词的 八头自注意力score 30个词 [128, 8, 30,30] \n",
    "\n",
    "\n",
    "\n",
    "QK可视化 score\n",
    "![vis-gpt](image/gpt2-self-attention-scoring-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d25344b",
   "metadata": {},
   "source": [
    "### 3.4.6 计算 emb_src->[layer normaliztion] ->multihead attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebad6f11",
   "metadata": {},
   "source": [
    "layer norm 公式\n",
    "class LayerNorm(nn.Module)\n",
    "![layer](image/layer_norm.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b578cb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer Normalization, 层归一化\n",
    "# models/layer/layer_norm.py\n",
    "# class LayerNorm(nn.Module)\n",
    "\n",
    "norm = model.encoder.layers[0].norm1\n",
    "print(norm)\n",
    "\n",
    "x = emb_src\n",
    "print(\"==============LayerNorm===========\")\n",
    "print(\"LayerNorm gamma: \", norm.gamma.shape)\n",
    "print(\"LayerNorm beta: \", norm.beta.shape)\n",
    "print(\"LayerNorm eps: \", norm.eps)\n",
    "\n",
    "mean = x.mean(-1, keepdim=True)\n",
    "print(\"LayerNorm mean: \", mean.shape)\n",
    "\n",
    "var = x.var(-1, unbiased=False, keepdim=True)\n",
    "print(\"LayerNorm var: \", var.shape)\n",
    "# '-1' means last dimension. \n",
    "\n",
    "out = (x - mean) / torch.sqrt(var + norm.eps)\n",
    "print(\"LayerNorm norm out: \", out.shape)\n",
    "\n",
    "out = norm.gamma * out + norm.beta\n",
    "print(\"LayerNorm norm out offset: \", out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8a81da",
   "metadata": {},
   "source": [
    "### 3.4.7 计算attention-> [position-wise-feed-forward]->layernorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc7edd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PositionwiseFeedForward, 位置前向传播\n",
    "# models/layer/position_wise_feed_forward.py\n",
    "# class PositionwiseFeedForward(nn.Module)\n",
    "print(\"PositionwiseFeedForward, 位置前向传播\")\n",
    "ffn = model.encoder.layers[0].ffn\n",
    "print(ffn)\n",
    "print(\"n_hidden: \", ffn_hidden)\n",
    "\n",
    "_x = emb_src\n",
    "print(\"1. before linear:\", _x.shape)\n",
    "\n",
    "_x = ffn.linear1(_x)\n",
    "print(\"2. after linear1:\", _x.shape)\n",
    "\n",
    "_x = ffn.relu(_x)\n",
    "_x = ffn.dropout(_x)\n",
    "_x = ffn.linear2(_x)\n",
    "print(\"3. after linear2:\", _x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a854423",
   "metadata": {},
   "source": [
    "### 3.4.8 计算enc_src+emb_trg->[decoder]->output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef53544",
   "metadata": {},
   "source": [
    "![decoder_dataflow_block](image/The_transformer_encoder_decoder_stack.png)\n",
    "![decoder_dataflow](image/transformer_resideual_layer_norm_3.png)\n",
    "![decoder_pipeline_single](image/transformer_decoding_2.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdfc33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder, 解码结构\n",
    "# models/model/decoder.py\n",
    "# class Decoder(nn.Module)\n",
    "print(\"解码层结构：\")\n",
    "# print(model.decoder)\n",
    "\n",
    "print(\"解码层输入target和编码层一样做embeding\")\n",
    "print(\"trg输入\", trg.shape)\n",
    "emb_trg = model.decoder.emb(trg)\n",
    "print(\"trg embding\", emb_trg.shape)\n",
    "print(\"解码层数:\", len(model.decoder.layers))\n",
    "for layer in model.decoder.layers:\n",
    "    # 注意这里需要有编码层的输入\n",
    "    decode_trg = layer(emb_trg, enc_src, trg_mask, src_trg_mask)\n",
    "print(\"编码层输出：\", decode_trg.shape)\n",
    "# pass to LM head\n",
    "output_decode = model.decoder.linear(decode_trg)\n",
    "print(\"编码层liner处理：\", output_decode.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484c43f6",
   "metadata": {},
   "source": [
    "### 3.4.9 计算[decoder block]: decode-self-attention -> enc-dec-attention ->ffn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a86d9f",
   "metadata": {},
   "source": [
    "![encoder-decoder](image/transformer_resideual_layer_norm_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a89454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecoderLayer, 解码层\n",
    "# models/blocks/decoder_layer.py\n",
    "# class DecoderLayer(nn.Module)\n",
    "\n",
    "# decode layer\n",
    "layer = model.decoder.layers[0]\n",
    "# print(\"decode layer结构：\")\n",
    "# print(layer)\n",
    "\n",
    "dec = emb_trg\n",
    "enc = enc_src\n",
    "_x = dec\n",
    "\n",
    "x = layer.self_attention(q=dec, k=dec, v=dec, mask=trg_mask)\n",
    "x = layer.dropout1(x)\n",
    "x = layer.norm1(x + _x)\n",
    "\n",
    "if enc is not None:\n",
    "    # 3. compute encoder - decoder attention\n",
    "    _x = x\n",
    "    # 多头注意力机制\n",
    "    print('q: trg_x:', x.shape)\n",
    "    print('k: enc:', enc.shape)\n",
    "    print('v: enc:', enc.shape)\n",
    "    print('mask: src_trg_mask:', src_trg_mask.shape)\n",
    "    x = layer.enc_dec_attention(q=x, k=enc, v=enc, mask=src_trg_mask)\n",
    "    print(\"enc->dec 注意力后: \", x.shape)\n",
    "    # 4. add and norm\n",
    "    x = layer.dropout2(x)\n",
    "    x = layer.norm2(x + _x)\n",
    "\n",
    "# 5. positionwise feed forward network\n",
    "_x = x\n",
    "x = layer.ffn(x)\n",
    "\n",
    "# 6. add and norm\n",
    "x = layer.dropout3(x)\n",
    "x = layer.norm3(x + _x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f091757",
   "metadata": {},
   "source": [
    "### 3.4.10 计算loss : output->[Cross Entropy loss]->logits->loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec5ade5",
   "metadata": {},
   "source": [
    "![loss](image/transformer_decoder_output_softmax.png)\n",
    "![loss_vocab](image/output_trained_model_probability_distributions.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24281c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 损失计算\n",
    "print('损失计算，使用交叉损失：')\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=src_pad_idx)\n",
    "print('src:', src.shape)\n",
    "print('trg:', trg.shape)\n",
    "print('trg[:, :-1]:', trg[:, :-1].shape)\n",
    "output = model(src, trg[:, :-1])\n",
    "print('output:', output.shape)\n",
    "output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "print('output_reshape:', output_reshape.shape)\n",
    "\n",
    "trg_view = trg[:, 1:].contiguous().view(-1)\n",
    "print('trg.view(-1):', trg_view.shape)\n",
    "loss = criterion(output_reshape, trg_view)\n",
    "print('loss:', loss)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886ffa35",
   "metadata": {},
   "source": [
    "### 3.4.11 编解码Mask计算原理enc-dec-mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaa9891",
   "metadata": {},
   "outputs": [],
   "source": [
    "## mask机制\n",
    "print(\"src_mask:\", src_mask.shape)\n",
    "print(\"src_trg_mask:\", src_trg_mask.shape)\n",
    "print(\"trg_mask:\", trg_mask.shape)\n",
    "# print(src_mask[0][0].int())\n",
    "# print(src_trg_mask[0][0].int())\n",
    "print(src_mask[0,0,:5,:5])\n",
    "print(trg_mask[0,0,:5,:5])\n",
    "print(src_trg_mask[0,0,:10,:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cefe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode-decode-mask\n",
    "layer = model.decoder.layers[0]\n",
    "# print(\"decode layer结构：\")\n",
    "# print(layer)\n",
    "\n",
    "dec = emb_trg\n",
    "enc = enc_src\n",
    "\n",
    "# _x = dec\n",
    "# 1. decode self attention for target \n",
    "# x = layer.self_attention(q=dec, k=dec, v=dec, mask=trg_mask)\n",
    "# x = layer.dropout1(x)\n",
    "# x = layer.norm1(x + _x)\n",
    "\n",
    "# 2. ecode-decode-attention + mask\n",
    "# if enc is not None:\n",
    "#     # 3. compute encoder - decoder attention\n",
    "#     _x = x\n",
    "#     # 多头注意力机制\n",
    "#     print('q: trg_x:', x.shape)\n",
    "#     print('k: enc:', enc.shape)\n",
    "#     print('v: enc:', enc.shape)\n",
    "#     print('mask: src_trg_mask:', src_trg_mask.shape)\n",
    "#     x = layer.enc_dec_attention(q=x, k=enc, v=enc, mask=src_trg_mask)\n",
    "\n",
    "# layer.enc_dec_attention 多头\n",
    "# layer.enc_dec_attention.attention() 单头\n",
    "\n",
    "\n",
    "q_dec = dec\n",
    "q = q_dec = layer.enc_dec_attention.split(dec)\n",
    "k = k_enc = _k_single\n",
    "v = v_enc = _v_single\n",
    "\n",
    "batch_size, head, length, d_tensor = k.size()\n",
    "\n",
    "# 1. dot product Query with Key^T to compute similarity\n",
    "k_t = k.transpose(2, 3)  # transpose\n",
    "\n",
    "\n",
    "print(\"q:\", q.shape)\n",
    "print(\"k_t:\", k_t.shape)\n",
    "score = (q @ k_t) / math.sqrt(d_tensor)  # scaled dot product\n",
    "print(\"score:\", score.shape)\n",
    "# 2. apply masking (opt)\n",
    "if src_trg_mask is not None: # 实际预测时，没有mask，会预测出终止标志符号\n",
    "    print(\"enc-dec-mask:\",src_trg_mask.shape)\n",
    "    score = score.masked_fill(src_trg_mask == 0, -10000)\n",
    "# 3. pass them softmax to make [0, 1] range\n",
    "score = attention.softmax(score)\n",
    "# 4. multiply with Value\n",
    "print(\"v:\", v.shape)\n",
    "v = score @ v\n",
    "print(\"score * v:\", v.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47227947",
   "metadata": {},
   "source": [
    "## 4. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabb62dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(params=model.parameters(),\n",
    "                 lr=init_lr,\n",
    "                 weight_decay=weight_decay,\n",
    "                 eps=adam_eps)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 verbose=True,\n",
    "                                                 factor=factor,\n",
    "                                                 patience=patience)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=src_pad_idx)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src \n",
    "        trg = batch.trg \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg[:, :-1])\n",
    "        output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "        trg = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(output_reshape, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        print('step :', round((i / len(iterator)) * 100, 2), '% , loss :', loss.item())\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eee504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_max = 100\n",
    "# iter_max = 1000\n",
    "train_losses = []\n",
    "for step in range(iter_max):\n",
    "        train_loss = train(model, train_iter, optimizer, criterion, clip)\n",
    "\n",
    "        if step > warmup:\n",
    "            scheduler.step(valid_loss)\n",
    "        train_losses.append(train_loss)\n",
    "        f = open('result/train_loss.txt', 'w')\n",
    "        f.write(str(train_losses))\n",
    "        f.close()\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "torch.save(model.state_dict(), 'model-final.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97565fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "print(train_losses)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(train_losses, 'r', label='train')\n",
    "plt.title('training result')\n",
    "plt.grid(True, which='both', axis='both')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef3a24c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad0aab0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

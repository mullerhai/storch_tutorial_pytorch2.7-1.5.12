{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9304fcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cd110aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src: torch.Size([2, 4])\n",
      "trg: torch.Size([2, 3])\n",
      "src batch max length: 3\n",
      "trg batch max length: 4\n"
     ]
    }
   ],
   "source": [
    "# padding\n",
    "k_pad_idx = 0\n",
    "q_pad_idx = 0\n",
    "\n",
    "# token序列 \n",
    "# src: 第一句话长度为3，第二句话长度为4， \n",
    "# 在这个batch中，batch_length=4，第一句话需要padding一个0 \n",
    "src_token = torch.tensor([[3, 4, 192, 0],[2, 8, 5, 3]]) \n",
    "\n",
    "# trg: 第一句话长度为2，第二句话长度为3\n",
    "# 在这个batch中，batch_length=3，第一句话需要padding一个0 \n",
    "trg_token = torch.tensor([[6, 7, 0],[11, 28, 9]])\n",
    "\n",
    "print(\"src:\", src_token.shape)\n",
    "print(\"trg:\", trg_token.shape)\n",
    "\n",
    "len_q, len_k = trg_token.size(1), src_token.size(1)\n",
    "print(\"src batch max length:\", len_q)\n",
    "print(\"trg batch max length:\", len_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae4fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeding\n",
    "# src = torch.randn(2, 4, 512) # 2个batch， 4个长度， 512维度\n",
    "# trg = torch.randn(2, 3, 512) # 2个batch， 3个长度， 512维度\n",
    "\n",
    "# 多头Q\n",
    "# src encode k : 2个batch， 8个头， 4个长度， 单头64维度\n",
    "src = torch.randn(2, 8, 4, 64) \n",
    "\n",
    "# trg decode Q : 2个batch， 8个头， 3个长度， 单头64维度\n",
    "trg = torch.randn(2, 8, 3, 64) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cb08196a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_token:\n",
      " tensor([[  3,   4, 192,   0],\n",
      "        [  2,   8,   5,   3]])\n",
      "trg_token:\n",
      " tensor([[8, 2, 0],\n",
      "        [5, 2, 2]])\n",
      "--------------------------------------------------------\n",
      "src_mask:\n",
      " tensor([[ True,  True,  True, False],\n",
      "        [ True,  True,  True,  True]])\n",
      "trg_mask:\n",
      " tensor([[ True,  True, False],\n",
      "        [ True,  True,  True]])\n",
      "--------------------------------------------------------\n",
      "【src_token】:torch.Size([2, 4])->【src_mask】: torch.Size([2, 1, 1, 4])\n",
      "【trg_token】:torch.Size([2, 3])->【trg_mask】: torch.Size([2, 1, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "# 为配合多头注意力，需要填充维度\n",
    "src_mask = src_token.ne(k_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "trg_mask = trg_token.ne(q_pad_idx).unsqueeze(1).unsqueeze(3)\n",
    "print(\"src_token:\\n\", src_token)\n",
    "print(\"trg_token:\\n\", trg_token)\n",
    "print('--------------------------------------------------------')\n",
    "print(\"src_mask:\\n\", src_token.ne(k_pad_idx))\n",
    "print(\"trg_mask:\\n\", trg_token.ne(q_pad_idx))\n",
    "print('--------------------------------------------------------')\n",
    "print(f\"【src_token】:{src_token.shape}->【src_mask】:\",src_mask.shape)\n",
    "print(f\"【trg_token】:{trg_token.shape}->【trg_mask】:\",trg_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2ce615c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【src_mask】:torch.Size([2, 1, 1, 4]) -> 【src_mask_repeat】:torch.Size([2, 1, 3, 4]) \n",
      "【trg_mask】:torch.Size([2, 1, 3, 1]) -> 【trg_mask_repeat】:torch.Size([2, 1, 3, 4]) \n"
     ]
    }
   ],
   "source": [
    "# 批量处理\n",
    "src_mask_repeat = src_mask.repeat(1, 1, len_q, 1)\n",
    "trg_mask_repeat = trg_mask.repeat(1, 1, 1, len_k)\n",
    "print(f\"【src_mask】:{src_mask.shape} -> 【src_mask_repeat】:{src_mask_repeat.shape} \")\n",
    "print(f\"【trg_mask】:{trg_mask.shape} -> 【trg_mask_repeat】:{trg_mask_repeat.shape} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ccef8d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "------------------------------\n",
      "src_mask \n",
      " tensor([[1, 1, 1, 0],\n",
      "        [1, 1, 1, 0],\n",
      "        [1, 1, 1, 0]], dtype=torch.int32)\n",
      "trg_mask \n",
      " tensor([[1, 1, 1, 1],\n",
      "        [1, 1, 1, 1],\n",
      "        [0, 0, 0, 0]], dtype=torch.int32)\n",
      "------------------------------\n",
      "mask \n",
      " tensor([[1, 1, 1, 0],\n",
      "        [1, 1, 1, 0],\n",
      "        [0, 0, 0, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# & and 操作符 1&1=1, 1&0=0, 0&1=0, 0&0=0\n",
    "mask = src_mask_repeat & trg_mask_repeat #[2,1,3,4] & [2,1,3,4] =[2,1,3,4]\n",
    "print(mask[0][0].shape)\n",
    "print('------------------------------')\n",
    "print(\"src_mask \\n\",src_mask_repeat[0][0].int())\n",
    "print(\"trg_mask \\n\", trg_mask_repeat[0][0].int())\n",
    "print('------------------------------')\n",
    "print(\"mask \\n\",mask[0][0].int())\n",
    "# token序列，q=[6, 7, 0], k=[3, 4, 192, 0]， 0为padding-index\n",
    "# 第一排 1，1，1，0 对应 6 与 3， 4， 192， 0之间的attention mask\n",
    "# 第二排 1，1，1，0 对应 7 与 3， 4， 192， 0之间的attention mask\n",
    "# 第三排 0，0，0，0 对应 0 与 3， 4， 192， 0之间的attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2f72bb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0, 0],\n",
      "        [1, 1, 0, 0],\n",
      "        [1, 1, 1, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Decode Self Masked \n",
    "# mask_decode = \n",
    "mask_decode = torch.tril(torch.ones(len_q, len_k)).type(torch.BoolTensor)\n",
    "print(mask_decode.int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "000c8fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 3, 64])\n",
      "tensor([[  1.1102,  -3.2859, -12.9273,     -inf],\n",
      "        [ -0.7410,  -2.2688,  14.2815,     -inf],\n",
      "        [    -inf,     -inf,     -inf,     -inf]])\n"
     ]
    }
   ],
   "source": [
    "# 计算 encode K， decode Q之间的注意力分数\n",
    "q = trg\n",
    "k_t = src.transpose(2,3)\n",
    "\n",
    "print(xq.shape)\n",
    "score = xq @ xk_t\n",
    "\n",
    "score_mask = score.masked_fill(mask == 0, -torch.inf)\n",
    "# score_mask = score.masked_fill(mask == 0, -10000)\n",
    "# print(score_mask)\n",
    "print(score_mask[0,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7402606b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7768f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

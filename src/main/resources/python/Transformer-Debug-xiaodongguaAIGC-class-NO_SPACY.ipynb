{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e65e6ce3",
   "metadata": {},
   "source": [
    "# 手撕Transformer-小冬瓜AIGC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861a9b75",
   "metadata": {},
   "source": [
    "![contetn](image/content.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d19b90",
   "metadata": {},
   "source": [
    "## 1 预处理requirements/configure/tokenizer/dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff511685",
   "metadata": {},
   "source": [
    "### 1.1 requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0934d833",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torchtext==0.6.0\n",
    "!pip3 install spacy\n",
    "!pip3 install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463da99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy download de_core_news_sm\n",
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4cedcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import spacy\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.optim import Adam\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54002aa",
   "metadata": {},
   "source": [
    "### 1.2 configure配置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fc6ea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 配置参数\n",
    "# GPU device setting\n",
    " \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 模型参数\n",
    "batch_size = 128 # 训练批次 句话\n",
    "max_len = 256    # 单句最大长度 \n",
    "##\n",
    "# padding=10\n",
    "\n",
    "d_model = 512    # 词嵌入向量维度\n",
    "n_layers = 6     # encoder/decoder层数量\n",
    "n_heads = 8      # 注意力头数： 假如有词嵌入维度d_model = 512 / n_heads = 8 => 单头向量维度 512 / 8 = 64，即QKV维度\n",
    "ffn_hidden = 2048 # 前向传播维度。 512 -> 2048 -> 512, 通常也称作proj\n",
    "drop_prob = 0.1  # dropout提升鲁棒性，随机失活一些节点\n",
    "n_hidden = ffn_hidden\n",
    "\n",
    "# optimizer parameter setting\n",
    "init_lr = 1e-5\n",
    "factor = 0.9\n",
    "adam_eps = 5e-9\n",
    "patience = 10\n",
    "warmup = 100\n",
    "epoch = 100\n",
    "clip = 1.0\n",
    "weight_decay = 5e-4\n",
    "inf = float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a35dc0",
   "metadata": {},
   "source": [
    "### 1.3 Tokenizer 英德文tokenzier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ba38b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example sentence.\n",
      "['This', 'is', 'an', 'example', 'sentence', '.']\n"
     ]
    }
   ],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.spacy_de = spacy.load('de_core_news_sm')\n",
    "        self.spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "    def tokenize_de(self, text):\n",
    "        return [tok.text for tok in self.spacy_de.tokenizer(text)]\n",
    "\n",
    "    def tokenize_en(self, text):\n",
    "        return [tok.text for tok in self.spacy_en.tokenizer(text)]\n",
    "        # example\n",
    "        # doc = nlp('This is an example sentence.')\n",
    "        # tokens = [token.text for token in doc]\n",
    "        # print(tokens)\n",
    "        # ['This', 'is', 'an', 'example', 'sentence', '.']\n",
    "\n",
    "# 加载Token\n",
    "tokenizer = Tokenizer()\n",
    "example = 'This is an example sentence.'\n",
    "tokens = tokenizer.tokenize_en(example)\n",
    "# tokenizer将句子按照单词分成list\n",
    "print(example)\n",
    "print(tokens)\n",
    "# ['This', 'is', 'an', 'example', 'sentence', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc0d5bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two young, white males are outside near many bushes\n",
      "['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes']\n"
     ]
    }
   ],
   "source": [
    "example = 'two young, white males are outside near many bushes'\n",
    "tokens = tokenizer.tokenize_en(example)\n",
    "print(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e0d071",
   "metadata": {},
   "source": [
    "### 1.4 Dataloader创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e843f242",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset initializing start\n",
      "\n",
      "--------0. 根据spacy mutli30k 创建数据集-------\n",
      "['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']\n",
      "['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.']\n",
      "29000\n",
      "1000\n",
      "1014\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import Field, BucketIterator\n",
    "from torchtext.datasets.translation import Multi30k\n",
    "class DataLoader:\n",
    "    source: Field = None\n",
    "    target: Field = None\n",
    "    def __init__(self, ext, tokenize_en, tokenize_de, init_token, eos_token):\n",
    "        self.ext = ext\n",
    "        self.tokenize_en = tokenize_en\n",
    "        self.tokenize_de = tokenize_de\n",
    "        self.init_token = init_token\n",
    "        self.eos_token = eos_token\n",
    "        print('dataset initializing start')\n",
    "\n",
    "    def make_dataset(self):\n",
    "        if self.ext == ('.de', '.en'):\n",
    "            self.source = Field(tokenize=self.tokenize_de, init_token=self.init_token, eos_token=self.eos_token,\n",
    "                                lower=True, batch_first=True)\n",
    "            self.target = Field(tokenize=self.tokenize_en, init_token=self.init_token, eos_token=self.eos_token,\n",
    "                                lower=True, batch_first=True)\n",
    "\n",
    "        elif self.ext == ('.en', '.de'):\n",
    "            # Field() 函数返回一个 Field 类的实例，该实例有以下常用方法\n",
    "            # build_vocab：根据数据集构建词汇表。\n",
    "            self.source = Field(tokenize=self.tokenize_en, init_token=self.init_token, eos_token=self.eos_token,\n",
    "                                lower=True, batch_first=True)\n",
    "            self.target = Field(tokenize=self.tokenize_de, init_token=self.init_token, eos_token=self.eos_token,\n",
    "                                lower=True, batch_first=True)\n",
    "        # 拆分数据集\n",
    "        train_data, valid_data, test_data = Multi30k.splits(exts=self.ext, fields=(self.source, self.target))\n",
    "        return train_data, valid_data, test_data\n",
    "\n",
    "    def build_vocab(self, train_data, min_freq):\n",
    "        self.source.build_vocab(train_data, min_freq=min_freq)\n",
    "        self.target.build_vocab(train_data, min_freq=min_freq)\n",
    "\n",
    "    def make_iter(self, train, validate, test, batch_size, device):\n",
    "        train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train, validate, test),\n",
    "                                                                              batch_size=batch_size,\n",
    "                                                                              device=device)\n",
    "        print('dataset initializing done')\n",
    "        return train_iterator, valid_iterator, test_iterator\n",
    "\n",
    "# 需要对整句加上句头句尾token [<sos>, 'This', 'is', 'an', 'example', 'sentence', '.',  <eos>] \n",
    "loader = DataLoader(ext=('.en', '.de'),\n",
    "                    tokenize_en=tokenizer.tokenize_en,\n",
    "                    tokenize_de=tokenizer.tokenize_de,\n",
    "                    init_token='<sos>',\n",
    "                    eos_token='<eos>')\n",
    "\n",
    "# 创建 source/target Field实例（包含数据）\n",
    "print('\\n--------0. 根据spacy mutli30k 创建数据集-------')\n",
    "train, valid, test = loader.make_dataset()\n",
    "print(train.examples[0].src)\n",
    "print(train.examples[0].trg)\n",
    "print(len(train.examples))\n",
    "print(len(test.examples))\n",
    "print(len(valid.examples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c99ad58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------1. 查看词表大小-------\n",
      "src vocab size: 5893\n",
      "trg vocab size: 7853\n"
     ]
    }
   ],
   "source": [
    "loader.build_vocab(train_data=train, min_freq=2)\n",
    "print('--------1. 查看词表大小-------')\n",
    "print('src vocab size:', len(loader.source.vocab.stoi)) \n",
    "print('trg vocab size:', len(loader.target.vocab.stoi)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abd0ce07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------2. 建立词表后，如何将单词转成token数值-------\n",
      "word \t -> \t token\n",
      "<sos> \t \t 2\n",
      "two \t \t 16\n",
      "young \t \t 24\n",
      ", \t \t 15\n",
      "<eos> \t \t 3\n",
      "<pad> \t \t 1\n"
     ]
    }
   ],
   "source": [
    "print('--------2. 建立词表后，如何将单词转成token数值-------')\n",
    "# print('查看词表:', loader.source.vocab.stoi)\n",
    "print('word \\t -> \\t token')\n",
    "print('<sos> \\t \\t',loader.source.vocab.stoi['<sos>'])\n",
    "print('two \\t \\t',loader.source.vocab.stoi['two'])\n",
    "print('young \\t \\t',loader.source.vocab.stoi['young'])\n",
    "print(', \\t \\t',loader.source.vocab.stoi[','])\n",
    "print('<eos> \\t \\t',loader.source.vocab.stoi['<eos>'])\n",
    "print('<pad> \\t \\t',loader.source.vocab.stoi['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8425a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset initializing done\n",
      "----3. 从迭代器中取一对，可见其开头为<sos>2, 结尾<eos>3， 剩余为<pad>1---------------\n",
      "padding的作用：一个batch中有不同的句子， 句子里最大句长为l, 小于l的句子都填充<pad>1\n",
      "tensor([  2,   4,  14, 346,  20,   4, 153,  28,  21, 115, 154, 107,   8,   5,\n",
      "          3,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1])\n",
      "tensor([   2,    8,   16,  169,    5,  164,   21,    9,   35,    5, 1259,  116,\n",
      "         441,    4,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1])\n"
     ]
    }
   ],
   "source": [
    "train_iter, valid_iter, test_iter = loader.make_iter(train, valid, test,\n",
    "                                                     batch_size=batch_size,\n",
    "                                                     device=device)\n",
    "print('----3. 从迭代器中取一对，可见其开头为<sos>2, 结尾<eos>3， 剩余为<pad>1---------------')\n",
    "print('padding的作用：一个batch中有不同的句子， 句子里最大句长为l, 小于l的句子都填充<pad>1')\n",
    "for batch in train_iter:\n",
    "    print(batch.src[0])\n",
    "    print(batch.trg[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8b221ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('----4. 以下词表参数也是模型中重要的部分----')\n",
    "# src_pad_idx = loader.source.vocab.stoi['<pad>']\n",
    "# trg_pad_idx = loader.target.vocab.stoi['<pad>']\n",
    "# trg_sos_idx = loader.target.vocab.stoi['<sos>']\n",
    "src_pad_idx = 1\n",
    "trg_pad_idx = 1\n",
    "trg_sos_idx = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b522b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "enc_voc_size = 5893\n",
    "dec_voc_size = 7853\n",
    "\n",
    "# enc_voc_size = len(loader.source.vocab)\n",
    "# print(\"嵌入层的输入参数 {} x 维度 {}\".format(enc_voc_size,d_model))\n",
    "# dec_voc_size = len(loader.target.vocab)\n",
    "# print(\"全链接层输出维度 {} x 输出词表{}：\".format(d_model,dec_voc_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b353a940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load src shape torch.Size([128, 36])\n",
      "load trg shape torch.Size([128, 38])\n"
     ]
    }
   ],
   "source": [
    "# 从data中获取数据\n",
    "# 仅运行一次，保证测试时使用同一组数据\n",
    "\n",
    "# for i, batch in enumerate(train_iter):\n",
    "#     src = batch.src\n",
    "#     trg = batch.trg\n",
    "#     print(\"save src shape:\",src.shape)\n",
    "#     print(\"save trg shape\",trg.shape)\n",
    "#     torch.save(src, 'tensor_src.pt')\n",
    "#     torch.save(trg, 'tensor_trg.pt')\n",
    "#     break\n",
    "\n",
    "test_src = torch.load('tensor_src.pt')\n",
    "test_trg = torch.load('tensor_trg.pt')\n",
    "print(\"load src shape\", test_src.shape)\n",
    "print(\"load trg shape\", test_trg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cac8423",
   "metadata": {},
   "source": [
    "## 1.5 评价指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a8e7c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def calculate_bleu(reference, candidate):\n",
    "    reference = [reference.split()]\n",
    "    candidate = candidate.split()\n",
    "    smoothing_function = nltk.translate.bleu_score.SmoothingFunction()\n",
    "    bleu_score = nltk.translate.bleu_score.sentence_bleu(reference, candidate, smoothing_function=smoothing_function.method1)\n",
    "    return bleu_score\n",
    "\n",
    "# 示例用法\n",
    "reference_sentence = \"The cat is on the mat\"\n",
    "# candidate_sentence = \"The cat is sitting on the mat\"\n",
    "candidate_sentence = \"The cat is on the mat\"\n",
    "bleu = calculate_bleu(reference_sentence, candidate_sentence)\n",
    "print(\"BLEU score:\", bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0d4bbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 score: 0.9230769181065088\n",
      "ROUGE-2 score: 0.7272727223140496\n",
      "ROUGE-L score: 0.9230769181065088\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "def calculate_rouge(reference, candidate):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(candidate, reference)\n",
    "    rouge_1 = scores[0]['rouge-1']['f']\n",
    "    rouge_2 = scores[0]['rouge-2']['f']\n",
    "    rouge_l = scores[0]['rouge-l']['f']\n",
    "    return rouge_1, rouge_2, rouge_l\n",
    "\n",
    "# 示例用法\n",
    "reference_summary = \"The cat is on the mat\"\n",
    "candidate_summary = \"The cat is sitting on the mat\"\n",
    "rouge_1, rouge_2, rouge_l = calculate_rouge(reference_summary, candidate_summary)\n",
    "print(\"ROUGE-1 score:\", rouge_1)\n",
    "print(\"ROUGE-2 score:\", rouge_2)\n",
    "print(\"ROUGE-L score:\", rouge_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b99a07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER score: 0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "import jiwer\n",
    "\n",
    "def calculate_wer(reference, candidate):\n",
    "    wer = jiwer.wer(reference, candidate)\n",
    "    return wer\n",
    "\n",
    "# 示例用法\n",
    "reference_transcription = \"The cat is on the mat\"\n",
    "candidate_transcription = \"The cat is sitting on the mat\"\n",
    "wer = calculate_wer(reference_transcription, candidate_transcription)\n",
    "print(\"WER score:\", wer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ad6f5e",
   "metadata": {},
   "source": [
    "## 2. 手撕Transformer模型\n",
    "\n",
    "这个章节主要理解模型构造的过程，第3章会自顶向下debug 数据流"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac40f53e",
   "metadata": {},
   "source": [
    "### 2.1.1 Token Embedding\n",
    "目的将1个token转成一串向量\n",
    "参照Word2Vec算法原理如下图示\n",
    "\n",
    "Embdding Vec\n",
    "数据类型流向 word(string) -> 【token(int) -> vec(list(float))】\n",
    "\n",
    "以下为两个词对应的vec进行比较， 越相近的向量，词性相同\n",
    "![0](image/embeddings-cosine-personality.png)\n",
    "\n",
    "Word2Vec embedding\n",
    "\n",
    "纵轴词表数量， 横轴vec词向量维度， 期望找出当前单词和右边相近的单词向量\n",
    "\n",
    "\n",
    "![1](image/word2vec-lookup-embeddings.png)\n",
    "\n",
    "SkipGram: \n",
    "\n",
    "假设\"我是小冬瓜\", 对于\"冬\"单词与\"小\"和\"瓜\"相近positive，与\"我\"间隔较远\n",
    "![2](image/skipgram-sliding-window-5.png)\n",
    "\n",
    "Data and model\n",
    "\n",
    "则对于\"冬\"则与\"冬-小\"和\"冬-瓜\"相近label则为1， 人为构造负样本\"冬-控\",\"冬-龙\",\"冬-抗\",\"冬-狼\"设置label为0\n",
    "![3](image/word2vec-training-example-2.png)\n",
    "\n",
    "根据所构造的样本，即可训练词表\n",
    "\n",
    "Train error\n",
    "![4](image/word2vec-training-update.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958e97e6",
   "metadata": {},
   "source": [
    "## embedding 实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7b3b09af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.weight torch.Size([14, 512])\n",
      "embedding.weight: tensor([ 0.3077,  1.6821, -0.8445,  1.7122,  0.6515,  0.2656,  0.5346,  0.7102,\n",
      "        -0.5206,  1.4002], grad_fn=<SliceBackward0>)\n",
      "tensor([ 1.6492,  0.6710, -0.0295, -0.8489,  1.3947, -0.5767,  0.7576,  0.4782,\n",
      "         0.4737, -0.3594], grad_fn=<SliceBackward0>)\n",
      "输入数据 torch.Size([3, 10])\n",
      "输入数据的embedding torch.Size([3, 10, 512])\n",
      "tensor([ 1.6492,  0.6710, -0.0295, -0.8489,  1.3947, -0.5767,  0.7576,  0.4782,\n",
      "         0.4737, -0.3594], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "embd_layer = torch.nn.Embedding(14, 512)\n",
    "print('embedding.weight', embd_layer.weight.shape)\n",
    "print('embedding.weight:', embd_layer.weight[3,:10])\n",
    "\n",
    "print(embd_layer.weight[4][:10])\n",
    "\n",
    "input_id = torch.tensor([[2, 4, 5, 6, 7, 8, 3, 1, 1, 1], \n",
    "                      [2, 4, 9, 10,11,12,13,3, 1, 1],\n",
    "                      [2, 6, 7, 8, 9, 10,11,12,13,3]])\n",
    "\n",
    "\n",
    "print(\"输入数据\",input_id.shape)\n",
    "print(\"输入数据的embedding\", embd_layer(input_id).shape)\n",
    "\n",
    "print(embd_layer(input_id)[0][1][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63c58949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding更多直接了解word2vec:\n",
      "按照以上理论可以直接，通过torch创建embedding表\n",
      "torch.Size([5893, 512])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "print(\"embedding更多直接了解word2vec:\")\n",
    "print(\"按照以上理论可以直接，通过torch创建embedding表\")\n",
    "a = nn.Embedding(enc_voc_size, d_model)\n",
    "# embedding_layer = nn.Embedding(14, 128)\n",
    "print(a.weight.shape) # 14 * 128\n",
    "print(input_id.shape) # \n",
    "x = a(input_id)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87437afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenEmbedding(5893, 512, padding_idx=1)\n",
      "TokenEmbedding(7853, 512, padding_idx=1)\n"
     ]
    }
   ],
   "source": [
    "# 创建Token embedding类\n",
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super(TokenEmbedding, self).__init__(vocab_size, d_model, padding_idx=1)\n",
    "        \n",
    "test_src_token = TokenEmbedding(enc_voc_size, d_model) #对 src：en 进行embedding\n",
    "test_trg_token = TokenEmbedding(dec_voc_size, d_model) #对 trg：de 进行embedding\n",
    "print(test_src_token) \n",
    "print(test_trg_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd393f6",
   "metadata": {},
   "source": [
    "### 2.1.2 position encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4576a629",
   "metadata": {},
   "source": [
    "Position 编码公式\n",
    "\n",
    "十进制13  ->  二进制(1,1,0,1) 这是一种位置编码向量: transformer中则使用连续函数描述向量的生成。\n",
    "\n",
    "可直接记住公式， 也可以尝试通俗理解以下过程\n",
    "\n",
    "(1,1,0,1)  两两成组 (1,1) (0,1) -> 4维/2=2组： 两组index为 i+1, i \n",
    "\n",
    "position encoding后为： (sin(13/(i+1)),cos/(13(i+1))、 ((sin(13/i),cos(13/i)))\n",
    "\n",
    "则最后 (1,1,0,1) ->  (sin(13/(i+1)),cos(13(i+1))、 ((sin(13/i),cos(13/i)))\n",
    "\n",
    "\n",
    "![title](image/positional_encoding.jpg)\n",
    "\n",
    "\n",
    "以下为一种可视化理解如何从p,i变量生成位置编码\n",
    "\n",
    "![pos](image/position_embeding_pos.png)\n",
    "![pos_i](image/Fhc4M.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2e4d4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 512])\n",
      "tensor([-0.5064, -0.8623,  0.8102,  0.5862, -0.9944,  0.1054,  0.4133, -0.9106,\n",
      "         0.7891,  0.6142, -0.5736,  0.8192, -0.9598, -0.2807, -0.3029, -0.9530,\n",
      "         0.4024, -0.9155,  0.7761, -0.6306,  0.9018, -0.4321,  0.9040, -0.4274,\n",
      "         0.7908, -0.6121,  0.4624, -0.8867, -0.1569, -0.9876, -0.8389, -0.5443,\n",
      "        -0.8985,  0.4391,  0.0994,  0.9950,  0.9971,  0.0763,  0.0795, -0.9968,\n",
      "        -0.9965,  0.0837,  0.3968,  0.9179,  0.6316, -0.7753, -0.9985, -0.0547,\n",
      "         0.6582,  0.7528, -0.0600, -0.9982, -0.4476,  0.8942,  0.7570, -0.6534,\n",
      "        -0.9037,  0.4281,  0.9573, -0.2891, -0.9663,  0.2576,  0.9428, -0.3334,\n",
      "        -0.8641,  0.5033,  0.6826, -0.7308, -0.3510,  0.9364, -0.1308, -0.9914,\n",
      "         0.6554,  0.7553, -0.9834, -0.1812,  0.8371, -0.5471, -0.1461,  0.9893,\n",
      "        -0.7031, -0.7111,  0.9773, -0.2120, -0.2734,  0.9619, -0.7682, -0.6402,\n",
      "         0.8635, -0.5043,  0.2464,  0.9692, -0.9994,  0.0346,  0.1163, -0.9932,\n",
      "         0.9787,  0.2055, -0.2364,  0.9717, -0.9773, -0.2120,  0.1337, -0.9910,\n",
      "         1.0000,  0.0018,  0.1804,  0.9836, -0.9157,  0.4018, -0.6388, -0.7694,\n",
      "         0.5250, -0.8511,  0.9832,  0.1826,  0.2260,  0.9741, -0.7772,  0.6292,\n",
      "        -0.9218, -0.3877, -0.1316, -0.9913,  0.7655, -0.6434,  0.9639,  0.2661,\n",
      "         0.3591,  0.9333, -0.5089,  0.8608, -0.9860,  0.1669, -0.7832, -0.6217,\n",
      "        -0.0908, -0.9959,  0.6361, -0.7716,  0.9917, -0.1283,  0.8267,  0.5626,\n",
      "         0.2694,  0.9630, -0.3922,  0.9199, -0.8698,  0.4933, -0.9936, -0.1127,\n",
      "        -0.7521, -0.6590, -0.2630, -0.9648,  0.2940, -0.9558,  0.7466, -0.6653,\n",
      "         0.9796, -0.2012,  0.9541,  0.2995,  0.7017,  0.7125,  0.3014,  0.9535,\n",
      "        -0.1482,  0.9890, -0.5543,  0.8323, -0.8473,  0.5312, -0.9885,  0.1510,\n",
      "        -0.9708, -0.2399, -0.8125, -0.5829, -0.5496, -0.8354, -0.2263, -0.9741,\n",
      "         0.1126, -0.9936,  0.4277, -0.9039,  0.6884, -0.7253,  0.8751, -0.4840,\n",
      "         0.9781, -0.2083,  0.9972,  0.0751,  0.9394,  0.3429,  0.8167,  0.5771,\n",
      "         0.6441,  0.7650,  0.4377,  0.8991,  0.2133,  0.9770, -0.0147,  0.9999,\n",
      "        -0.2340,  0.9722, -0.4349,  0.9005, -0.6100,  0.7924, -0.7545,  0.6563,\n",
      "        -0.8658,  0.5004, -0.9431,  0.3326, -0.9871,  0.1599, -0.9999, -0.0117,\n",
      "        -0.9842, -0.1768, -0.9434, -0.3316, -0.8811, -0.4729, -0.8011, -0.5986,\n",
      "        -0.7070, -0.7072, -0.6024, -0.7982, -0.4904, -0.8715, -0.3741, -0.9274,\n",
      "        -0.2560, -0.9667, -0.1383, -0.9904, -0.0228, -0.9997,  0.0889, -0.9960,\n",
      "         0.1956, -0.9807,  0.2964, -0.9551,  0.3907, -0.9205,  0.4778, -0.8785,\n",
      "         0.5577, -0.8301,  0.6301, -0.7765,  0.6952, -0.7189,  0.7529, -0.6581,\n",
      "         0.8036, -0.5951,  0.8476, -0.5307,  0.8851, -0.4654,  0.9165, -0.4000,\n",
      "         0.9423, -0.3348,  0.9627, -0.2704,  0.9783, -0.2072,  0.9894, -0.1453,\n",
      "         0.9964, -0.0850,  0.9996, -0.0266,  0.9996,  0.0298,  0.9964,  0.0842,\n",
      "         0.9907,  0.1364,  0.9825,  0.1864,  0.9722,  0.2342,  0.9601,  0.2798,\n",
      "         0.9464,  0.3231,  0.9313,  0.3643,  0.9150,  0.4034,  0.8978,  0.4404,\n",
      "         0.8797,  0.4754,  0.8610,  0.5085,  0.8418,  0.5397,  0.8222,  0.5692,\n",
      "         0.8023,  0.5969,  0.7823,  0.6230,  0.7621,  0.6475,  0.7419,  0.6705,\n",
      "         0.7218,  0.6921,  0.7018,  0.7124,  0.6819,  0.7314,  0.6623,  0.7492,\n",
      "         0.6429,  0.7659,  0.6238,  0.7816,  0.6050,  0.7962,  0.5866,  0.8099,\n",
      "         0.5685,  0.8227,  0.5508,  0.8346,  0.5335,  0.8458,  0.5166,  0.8562,\n",
      "         0.5000,  0.8660,  0.4839,  0.8751,  0.4682,  0.8836,  0.4530,  0.8915,\n",
      "         0.4381,  0.8989,  0.4236,  0.9058,  0.4096,  0.9123,  0.3959,  0.9183,\n",
      "         0.3827,  0.9239,  0.3698,  0.9291,  0.3573,  0.9340,  0.3452,  0.9385,\n",
      "         0.3335,  0.9427,  0.3222,  0.9467,  0.3112,  0.9503,  0.3005,  0.9538,\n",
      "         0.2902,  0.9570,  0.2803,  0.9599,  0.2706,  0.9627,  0.2613,  0.9653,\n",
      "         0.2522,  0.9677,  0.2435,  0.9699,  0.2351,  0.9720,  0.2269,  0.9739,\n",
      "         0.2190,  0.9757,  0.2114,  0.9774,  0.2040,  0.9790,  0.1969,  0.9804,\n",
      "         0.1901,  0.9818,  0.1834,  0.9830,  0.1770,  0.9842,  0.1708,  0.9853,\n",
      "         0.1648,  0.9863,  0.1591,  0.9873,  0.1535,  0.9882,  0.1481,  0.9890,\n",
      "         0.1429,  0.9897,  0.1379,  0.9904,  0.1330,  0.9911,  0.1284,  0.9917,\n",
      "         0.1239,  0.9923,  0.1195,  0.9928,  0.1153,  0.9933,  0.1112,  0.9938,\n",
      "         0.1073,  0.9942,  0.1035,  0.9946,  0.0999,  0.9950,  0.0964,  0.9953,\n",
      "         0.0930,  0.9957,  0.0897,  0.9960,  0.0865,  0.9962,  0.0835,  0.9965,\n",
      "         0.0806,  0.9968,  0.0777,  0.9970,  0.0750,  0.9972,  0.0723,  0.9974,\n",
      "         0.0698,  0.9976,  0.0673,  0.9977,  0.0649,  0.9979,  0.0626,  0.9980,\n",
      "         0.0604,  0.9982,  0.0583,  0.9983,  0.0562,  0.9984,  0.0543,  0.9985,\n",
      "         0.0523,  0.9986,  0.0505,  0.9987,  0.0487,  0.9988,  0.0470,  0.9989,\n",
      "         0.0453,  0.9990,  0.0437,  0.9990,  0.0422,  0.9991,  0.0407,  0.9992,\n",
      "         0.0393,  0.9992,  0.0379,  0.9993,  0.0365,  0.9993,  0.0352,  0.9994,\n",
      "         0.0340,  0.9994,  0.0328,  0.9995,  0.0316,  0.9995,  0.0305,  0.9995,\n",
      "         0.0294,  0.9996,  0.0284,  0.9996,  0.0274,  0.9996,  0.0264,  0.9997])\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len, device):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model, device=device)\n",
    "        self.encoding.requires_grad = False  \n",
    "        pos = torch.arange(0, max_len, device=device)\n",
    "        pos = pos.float().unsqueeze(dim=1)\n",
    "        _2i = torch.arange(0, d_model, step=2, device=device).float()\n",
    "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
    "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
    "        # 512\n",
    "        # 2x256 cos sin\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        return self.encoding[:seq_len, :]\n",
    "\n",
    "test_pos_encoding = PositionalEncoding(d_model, max_len, device)\n",
    "print(test_pos_encoding.encoding.shape)\n",
    "print(test_pos_encoding.encoding[255,:]) # 255 is position \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb372ef9",
   "metadata": {},
   "source": [
    "### 2.1.3 LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83ee253",
   "metadata": {},
   "source": [
    "layer norm 公式\n",
    "\n",
    "原图公式与主要四行代码一一对应\n",
    "\n",
    "layernorm作用在最后一维进行归一化\n",
    "\n",
    "![layer](image/layer_norm.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5da48a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512])\n",
      "torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-12):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # layernorm作用在(-1) 最后一维进行归一化\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        var = x.var(-1, unbiased=False, keepdim=True)\n",
    "        out = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        out = self.gamma * out + self.beta\n",
    "        return out\n",
    "    \n",
    "test_ln = LayerNorm(d_model)\n",
    "print(test_ln.gamma.shape)\n",
    "print(test_ln.beta.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dad0e64",
   "metadata": {},
   "source": [
    "### 2.1.4 Scaled-Dot-Production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbbcd37",
   "metadata": {},
   "source": [
    "scaled dot product 图示\n",
    "class ScaleDotProductAttention(nn.Module)\n",
    "![attention](image/scale_dot_product_attention.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b6bee67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单头注意力机制\n",
    "# 图-代码-公式完全对应， 第3章节有详细推导\n",
    "# 先记住实现\n",
    "class ScaleDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaleDotProductAttention, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None, e=1e-12):\n",
    "        batch_size, head, length, d_tensor = k.size() # /n_embd/8\n",
    "        k_t = k.transpose(2, 3) \n",
    "        score = (q @ k_t) / math.sqrt(d_tensor) #qk^t/dk\n",
    "        if mask is not None:\n",
    "            score = score.masked_fill(mask == 0, -10000)\n",
    "        score = self.softmax(score) #softmax(qk^t/dk)\n",
    "        v = score @ v #softmax(qk^t/dk)*V\n",
    "        return v, score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db49dfb4",
   "metadata": {},
   "source": [
    "### 2.2.1 position wise feed forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1366cc6c",
   "metadata": {},
   "source": [
    "ffn\n",
    "![layer](image/positionwise_feed_forward.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41450fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PositionwiseFeedForward(\n",
      "  (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 前向传播，当成神经网络全链接层 + 隐含层理解\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "ffw = PositionwiseFeedForward(d_model, ffn_hidden)\n",
    "print(ffw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46237acc",
   "metadata": {},
   "source": [
    "### 2.2.2 Multi-Head-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95acfcb8",
   "metadata": {},
   "source": [
    "multi-head-attention\n",
    "![multiheadattention](image/multi_head_attention.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc59d1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadAttention(\n",
      "  (attention): ScaleDotProductAttention(\n",
      "    (softmax): Softmax(dim=-1)\n",
      "  )\n",
      "  (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
      ")\n",
      "512 8\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    # dmodel_n_embed; 512 8\n",
    "    def __init__(self, d_model, n_head):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.attention = ScaleDotProductAttention()\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_concat = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)  # 对应图里liner：先对QKV投影\n",
    "        q, k, v = self.split(q), self.split(k), self.split(v) # Q->Q0, Q1, ... \n",
    "        out, attention = self.attention(q, k, v, mask=mask) # 每一头计算attention，z0, z1, ...\n",
    "        out = self.concat(out) # 将每一头拼接 z0 z1 .. = z\n",
    "        out = self.w_concat(out) # z -> linner -> output\n",
    "        return out\n",
    "\n",
    "    # 先不用看实现，后面会讲\n",
    "    def split(self, tensor):\n",
    "        batch_size, length, d_model = tensor.size()\n",
    "        d_tensor = d_model // self.n_head\n",
    "        tensor = tensor.view(batch_size, length, self.n_head, d_tensor).transpose(1, 2)\n",
    "        return tensor\n",
    "\n",
    "    # 先不用看实现，后面会讲\n",
    "    def concat(self, tensor):\n",
    "        batch_size, head, length, d_tensor = tensor.size()\n",
    "        d_model = head * d_tensor\n",
    "        tensor = tensor.transpose(1, 2).contiguous().view(batch_size, length, d_model)\n",
    "        return tensor\n",
    "    \n",
    "test_multihead_attention = MultiHeadAttention(d_model, n_heads)\n",
    "print(test_multihead_attention)\n",
    "print(d_model, n_heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a144278",
   "metadata": {},
   "source": [
    "### 2.2.3 Transformer Embeding\n",
    "\n",
    "model.png：见input后的操作符token+position\n",
    "\n",
    "![model](image/model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fbe6f68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerEmbedding(\n",
      "  (tok_emb): TokenEmbedding(5893, 512, padding_idx=1)\n",
      "  (pos_emb): PositionalEncoding()\n",
      "  (drop_out): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Transformer—embedding数据流：【嵌入向量+位置编码 ->  X】 -> QKV -> X\n",
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len, drop_prob, device):\n",
    "        super(TransformerEmbedding, self).__init__()\n",
    "        self.tok_emb = TokenEmbedding(vocab_size, d_model)\n",
    "        self.pos_emb = PositionalEncoding(d_model, max_len, device)\n",
    "        self.drop_out = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tok_emb = self.tok_emb(x)\n",
    "        pos_emb = self.pos_emb(x)\n",
    "        # 记住这里还有个Dropout\n",
    "        return self.drop_out(tok_emb + pos_emb)\n",
    "    \n",
    "test_embedding = TransformerEmbedding(enc_voc_size, d_model, max_len, drop_prob, device)\n",
    "print(test_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66825129",
   "metadata": {},
   "source": [
    "### 2.3.1 Transformer Encode Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab7d373",
   "metadata": {},
   "source": [
    "编解码：enc-dec\n",
    "\n",
    "特别注意【每个 decoder block】都需要接受encoder的输出\n",
    "\n",
    "![enc-dec](image/enc_dec.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e90e9221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderLayer(\n",
      "  (attention): MultiHeadAttention(\n",
      "    (attention): ScaleDotProductAttention(\n",
      "      (softmax): Softmax(dim=-1)\n",
      "    )\n",
      "    (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (norm1): LayerNorm()\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (ffn): PositionwiseFeedForward(\n",
      "    (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (norm2): LayerNorm()\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 单独一个encoder block\n",
    "# 多个 encoder block 组成一个 encoder\n",
    "\n",
    "# 可以叫encoder-layer 也可以叫 encoder-block\n",
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
    "        self.norm1 = LayerNorm(d_model=d_model)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm2 = LayerNorm(d_model=d_model)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x, s_mask):\n",
    "        # 1. compute self attention\n",
    "        # print(\"encoder layer x: \", x.shape)\n",
    "        _x = x\n",
    "        x = self.attention(q=x, k=x, v=x, mask=s_mask)\n",
    "        \n",
    "        # 2. add and norm\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + _x)\n",
    "        \n",
    "        # 3. positionwise feed forward network\n",
    "        _x = x\n",
    "        x = self.ffn(x)\n",
    "      \n",
    "        # 4. add and norm\n",
    "        x = self.dropout2(x)\n",
    "        x = self.norm2(x + _x)\n",
    "        return x\n",
    "test_encoder_block = EncoderLayer(d_model, ffn_hidden, n_heads, drop_prob)\n",
    "print(test_encoder_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee7c12b",
   "metadata": {},
   "source": [
    "### 2.3.2 Transformer Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dd4e023f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecoderLayer(\n",
      "  (self_attention): MultiHeadAttention(\n",
      "    (attention): ScaleDotProductAttention(\n",
      "      (softmax): Softmax(dim=-1)\n",
      "    )\n",
      "    (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (norm1): LayerNorm()\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (enc_dec_attention): MultiHeadAttention(\n",
      "    (attention): ScaleDotProductAttention(\n",
      "      (softmax): Softmax(dim=-1)\n",
      "    )\n",
      "    (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (norm2): LayerNorm()\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  (ffn): PositionwiseFeedForward(\n",
      "    (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (norm3): LayerNorm()\n",
      "  (dropout3): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
    "        self.norm1 = LayerNorm(d_model=d_model)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        # enc_dec_attention使用encoder的 Q， decoder的 K，V\n",
    "        self.enc_dec_attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
    "        self.norm2 = LayerNorm(d_model=d_model)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm3 = LayerNorm(d_model=d_model)\n",
    "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, dec, enc, t_mask, s_mask):\n",
    "        # 1. compute self attention\n",
    "        _x = dec\n",
    "        x = self.self_attention(q=dec, k=dec, v=dec, mask=t_mask)#下三角矩阵\n",
    "        \n",
    "        # 2. add and norm\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + _x)\n",
    "\n",
    "        if enc is not None:\n",
    "            # 3. compute encoder - decoder attention\n",
    "            _x = x\n",
    "            x = self.enc_dec_attention(q=x, k=enc, v=enc, mask=s_mask) # \n",
    "            \n",
    "            # 4. add and norm\n",
    "            x = self.dropout2(x)\n",
    "            x = self.norm2(x + _x)\n",
    "\n",
    "        # 5. positionwise feed forward network\n",
    "        _x = x\n",
    "        x = self.ffn(x)\n",
    "        \n",
    "        # 6. add and norm\n",
    "        x = self.dropout3(x)\n",
    "        x = self.norm3(x + _x)\n",
    "        return x\n",
    "test_decoder_block = DecoderLayer(d_model, ffn_hidden, n_heads, drop_prob)\n",
    "print(test_decoder_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814af91f",
   "metadata": {},
   "source": [
    "### 2.3.3 Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d14b6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder block size :  6\n",
      "Encoder(\n",
      "  (emb): TransformerEmbedding(\n",
      "    (tok_emb): TokenEmbedding(5893, 512, padding_idx=1)\n",
      "    (pos_emb): PositionalEncoding()\n",
      "    (drop_out): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (layers): ModuleList(\n",
      "    (0-5): 6 x EncoderLayer(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (attention): ScaleDotProductAttention(\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (relu): ReLU()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, enc_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device):\n",
    "        super().__init__()\n",
    "        self.emb = TransformerEmbedding(d_model=d_model,\n",
    "                                        max_len=max_len,\n",
    "                                        vocab_size=enc_voc_size,\n",
    "                                        drop_prob=drop_prob,\n",
    "                                        device=device)\n",
    "\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model=d_model,\n",
    "                                                  ffn_hidden=ffn_hidden,\n",
    "                                                  n_head=n_head,\n",
    "                                                  drop_prob=drop_prob)\n",
    "                                     for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x, s_mask):\n",
    "        x = self.emb(x)\n",
    "        # 每个encoder block的输入输出tensor是一致的\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, s_mask)\n",
    "        return x\n",
    "    \n",
    "\n",
    "test_encoder = Encoder(enc_voc_size, max_len, d_model, ffn_hidden, n_heads, n_layers, drop_prob, device)\n",
    "print(\"encoder block size : \", len(test_encoder.layers))\n",
    "print(test_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc0cbb0",
   "metadata": {},
   "source": [
    "### 2.3.4 Transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4ecf1d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder block size :  6\n",
      "Encoder(\n",
      "  (emb): TransformerEmbedding(\n",
      "    (tok_emb): TokenEmbedding(5893, 512, padding_idx=1)\n",
      "    (pos_emb): PositionalEncoding()\n",
      "    (drop_out): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (layers): ModuleList(\n",
      "    (0-5): 6 x EncoderLayer(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (attention): ScaleDotProductAttention(\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (ffn): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (relu): ReLU()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (norm2): LayerNorm()\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dec_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device):\n",
    "        super().__init__()\n",
    "        self.emb = TransformerEmbedding(d_model=d_model,\n",
    "                                        drop_prob=drop_prob,\n",
    "                                        max_len=max_len,\n",
    "                                        vocab_size=dec_voc_size,\n",
    "                                        device=device)\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model=d_model,\n",
    "                                                  ffn_hidden=ffn_hidden,\n",
    "                                                  n_head=n_head,\n",
    "                                                  drop_prob=drop_prob)\n",
    "                                     for _ in range(n_layers)])\n",
    "\n",
    "        self.linear = nn.Linear(d_model, dec_voc_size)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        trg = self.emb(trg)\n",
    "\n",
    "        # 这里的每个layer，都有decoder的enc_src输入\n",
    "        for layer in self.layers:\n",
    "            trg = layer(trg, enc_src, trg_mask, src_mask) # src_trg_mask\n",
    "\n",
    "        # pass to LM head\n",
    "        output = self.linear(trg)\n",
    "        return output\n",
    "\n",
    "test_decoder = Decoder(dec_voc_size, max_len, d_model, ffn_hidden, n_heads, n_layers, drop_prob, device)\n",
    "print(\"decoder block size : \", len(test_decoder.layers))\n",
    "print(test_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bcdea6",
   "metadata": {},
   "source": [
    "### 2.4 Transformer结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5397e67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完整的Transfomer 类， 创建encoder / decoder\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, src_pad_idx, trg_pad_idx, trg_sos_idx, enc_voc_size, dec_voc_size, d_model, n_head, max_len,\n",
    "                 ffn_hidden, n_layers, drop_prob, device):\n",
    "        super().__init__()\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.trg_sos_idx = trg_sos_idx\n",
    "        self.device = device\n",
    "        self.encoder = Encoder(d_model=d_model,\n",
    "                               n_head=n_head,\n",
    "                               max_len=max_len,\n",
    "                               ffn_hidden=ffn_hidden,\n",
    "                               enc_voc_size=enc_voc_size,\n",
    "                               drop_prob=drop_prob,\n",
    "                               n_layers=n_layers,\n",
    "                               device=device)\n",
    "\n",
    "        self.decoder = Decoder(d_model=d_model,\n",
    "                               n_head=n_head,\n",
    "                               max_len=max_len,\n",
    "                               ffn_hidden=ffn_hidden,\n",
    "                               dec_voc_size=dec_voc_size,\n",
    "                               drop_prob=drop_prob,\n",
    "                               n_layers=n_layers,\n",
    "                               device=device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_pad_mask(src, src, self.src_pad_idx, self.src_pad_idx)\n",
    "\n",
    "        src_trg_mask = self.make_pad_mask(trg, src, self.trg_pad_idx, self.src_pad_idx)\n",
    "\n",
    "        trg_mask = self.make_pad_mask(trg, trg, self.trg_pad_idx, self.trg_pad_idx) * \\\n",
    "                   self.make_no_peak_mask(trg, trg)\n",
    "        # encoder计算流程 src -> encoder -> enc_src\n",
    "        # decoder计算流程 enc_src + trg -> decoder  -> output\n",
    "        # 关于Mask后面会讲解\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        output = self.decoder(trg, enc_src, trg_mask, src_trg_mask)\n",
    "        return output\n",
    "\n",
    "    def make_pad_mask(self, q, k, q_pad_idx, k_pad_idx):\n",
    "        len_q, len_k = q.size(1), k.size(1)\n",
    "\n",
    "        # batch_size x 1 x 1 x len_k\n",
    "        k = k.ne(k_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # batch_size x 1 x len_q x len_k\n",
    "        k = k.repeat(1, 1, len_q, 1)\n",
    "\n",
    "        # batch_size x 1 x len_q x 1\n",
    "        q = q.ne(q_pad_idx).unsqueeze(1).unsqueeze(3)\n",
    "        # batch_size x 1 x len_q x len_k\n",
    "        q = q.repeat(1, 1, 1, len_k)\n",
    "\n",
    "        mask = k & q\n",
    "        return mask\n",
    "\n",
    "    def make_no_peak_mask(self, q, k):\n",
    "        len_q, len_k = q.size(1), k.size(1)\n",
    "        # len_q x len_k\n",
    "        mask = torch.tril(torch.ones(len_q, len_k)).type(torch.BoolTensor).to(self.device)\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c6828f",
   "metadata": {},
   "source": [
    "## 3. 调试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d94ccb1",
   "metadata": {},
   "source": [
    "### 3.1 创建Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "723837b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nb/5w4m7xtn6vs7ncl5gs5fs9600000gn/T/ipykernel_20087/3949434347.py:19: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  nn.init.kaiming_uniform(m.weight.data)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (emb): TransformerEmbedding(\n",
       "      (tok_emb): TokenEmbedding(5893, 512, padding_idx=1)\n",
       "      (pos_emb): PositionalEncoding()\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x EncoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (emb): TransformerEmbedding(\n",
       "      (tok_emb): TokenEmbedding(7853, 512, padding_idx=1)\n",
       "      (pos_emb): PositionalEncoding()\n",
       "      (drop_out): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x DecoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (enc_dec_attention): MultiHeadAttention(\n",
       "          (attention): ScaleDotProductAttention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm3): LayerNorm()\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (linear): Linear(in_features=512, out_features=7853, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformer为./models/transformer.py里的模型类，包含多个对象和方法\n",
    "\n",
    "model = Transformer(src_pad_idx=src_pad_idx,\n",
    "                    trg_pad_idx=trg_pad_idx,\n",
    "                    trg_sos_idx=trg_sos_idx,\n",
    "                    d_model=d_model,\n",
    "                    enc_voc_size=enc_voc_size,\n",
    "                    dec_voc_size=dec_voc_size,\n",
    "                    max_len=max_len,\n",
    "                    ffn_hidden=ffn_hidden,\n",
    "                    n_head=n_heads,\n",
    "                    n_layers=n_layers,\n",
    "                    drop_prob=drop_prob,\n",
    "                    device=device).to(device)\n",
    "\n",
    "# 使用kaiming_uniform对model初始化\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.kaiming_uniform(m.weight.data)\n",
    "        \n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc37af7b",
   "metadata": {},
   "source": [
    "### 3.2 创建调试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a769fcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load src shape torch.Size([128, 36])\n",
      "load trg shape torch.Size([128, 38])\n"
     ]
    }
   ],
   "source": [
    "# # 从data中获取数据\n",
    "# for i, batch in enumerate(train_iter):\n",
    "#     src = batch.src\n",
    "#     trg = batch.trg\n",
    "#     print(\"save src shape:\",src.shape)\n",
    "#     print(\"save trg shape\",trg.shape)\n",
    "#     torch.save(src, 'tensor_src.pt')\n",
    "#     torch.save(trg, 'tensor_trg.pt')\n",
    "#     break\n",
    "\n",
    "test_src = torch.load('tensor_src.pt')\n",
    "test_trg = torch.load('tensor_trg.pt')\n",
    "print(\"load src shape\", test_src.shape)\n",
    "print(\"load trg shape\", test_trg.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a34ec58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load src shape torch.Size([128, 36])\n",
      "load trg shape torch.Size([128, 38])\n",
      "batch size : 128 and src length: 36 \n",
      "batch size : 128 and trg length: 38 \n",
      "src [0]:  tensor([   2,   30, 1622,   58,   16,   30,   17,    6,    4,  565, 1028,    5,\n",
      "           3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1])\n",
      "trg [0]:  tensor([  2,  30, 185,  23,   9,  35,  18,  30,  20,   0,  52,   4,   3,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1])\n",
      "src_pad_idx: 1\n",
      "trg_pad_idx: 1\n",
      "trg_sos_idx: 2\n"
     ]
    }
   ],
   "source": [
    "# 加载数据集, 从dataloader中获取\n",
    "# 接下来所有数据计算，都基于batch(128)\n",
    "\n",
    "src = torch.load('tensor_src.pt')\n",
    "trg = torch.load('tensor_trg.pt')\n",
    "print(\"load src shape\", src.shape)\n",
    "print(\"load trg shape\", trg.shape)\n",
    "print('batch size : {} and src length: {} '.format(src.shape[0], src.shape[1]))\n",
    "print('batch size : {} and trg length: {} '.format(trg.shape[0], trg.shape[1]))\n",
    "print('src [0]: ', src[0])\n",
    "print('trg [0]: ', trg[0])\n",
    "print('src_pad_idx:',src_pad_idx)\n",
    "print('trg_pad_idx:',trg_pad_idx)\n",
    "print('trg_sos_idx:',trg_sos_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a131f78",
   "metadata": {},
   "source": [
    "### 3.3 创建mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4be21417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_mask: torch.Size([128, 1, 36, 36])\n",
      "src_trg_mask: torch.Size([128, 1, 38, 36])\n",
      "trg_mask: torch.Size([128, 1, 38, 38])\n"
     ]
    }
   ],
   "source": [
    "# 根据pad信息，创建mask，先忽略实现细节\n",
    "src_mask = model.make_pad_mask(src, src, src_pad_idx, src_pad_idx)\n",
    "src_trg_mask = model.make_pad_mask(trg, src, trg_pad_idx, src_pad_idx)\n",
    "trg_mask = model.make_pad_mask(trg, trg, trg_pad_idx, trg_pad_idx) * \\\n",
    "            model.make_no_peak_mask(trg, trg)\n",
    "print(\"src_mask:\", src_mask.shape)\n",
    "print(\"src_trg_mask:\", src_trg_mask.shape)\n",
    "print(\"trg_mask:\", trg_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c28a7e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(src_mask[0][0].int())\n",
    "# print(src_trg_mask[0][0].int())\n",
    "# # trg.Q.shape() * src.K^T.shape()\n",
    "# print(trg_mask[0][0].int()) # 下三角"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8575e6",
   "metadata": {},
   "source": [
    "### 3.4 图解Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa48f39",
   "metadata": {},
   "source": [
    "![all](image/the_transformer_3.png)\n",
    "![all](image/The_transformer_encoders_decoders.png)\n",
    "![all](image/The_transformer_encoder_decoder_stack.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be24580",
   "metadata": {},
   "source": [
    "### 3.4.1 计算src->[encoder->decoder]->target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e8f722",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "419f50d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 36])\n",
      "torch.Size([128, 36, 512])\n",
      "torch.Size([128, 38, 7853])\n",
      "decode voc size: 7853\n",
      "d_model: 512\n"
     ]
    }
   ],
   "source": [
    "# # transformer 编码层和解码层计算\n",
    "# print(\"查看模型：\", model)\n",
    "enc_src = model.encoder(src, src_mask)\n",
    "output = model.decoder(trg, enc_src, trg_mask, src_trg_mask)\n",
    "print(src.shape)\n",
    "print(enc_src.shape)\n",
    "print(output.shape)\n",
    "print(\"decode voc size:\", dec_voc_size)\n",
    "print(\"d_model:\", d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e193a7",
   "metadata": {},
   "source": [
    "![embedding](image/transformer_positional_encoding_vectors.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3b1ae0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src: torch.Size([128, 36, 512])\n",
      "emb_src: torch.Size([128, 36, 512])\n",
      "n_layers: 6\n",
      "encode layers: 6\n",
      "encoder_src: torch.Size([128, 36, 512])\n",
      "encoder_src: torch.Size([128, 36, 512])\n",
      "encoder_src: torch.Size([128, 36, 512])\n",
      "encoder_src: torch.Size([128, 36, 512])\n",
      "encoder_src: torch.Size([128, 36, 512])\n",
      "encoder_src: torch.Size([128, 36, 512])\n"
     ]
    }
   ],
   "source": [
    "# encoder 编码层计算\n",
    "# encoder包含emb和n_layers层\n",
    "\n",
    "emb_src = model.encoder.emb(src)\n",
    "print('src:', emb_src.shape)\n",
    "print('emb_src:', emb_src.shape)\n",
    "print('n_layers:', n_layers)\n",
    "print('encode layers:', len(model.encoder.layers))\n",
    "# encoder0 -> encoder1\n",
    "for layer in model.encoder.layers:\n",
    "    encoder_src = layer(emb_src, src_mask)\n",
    "    print('encoder_src:', encoder_src.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0231654",
   "metadata": {},
   "source": [
    "### 3.4.2 计算input->embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44d52b7",
   "metadata": {},
   "source": [
    "数值position\n",
    "![embedding-sample](image/transformer_positional_encoding_example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9406dae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerEmbedding(\n",
      "  (tok_emb): TokenEmbedding(5893, 512, padding_idx=1)\n",
      "  (pos_emb): PositionalEncoding()\n",
      "  (drop_out): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "src: torch.Size([128, 36])\n",
      "tok_emb: torch.Size([128, 36, 512])\n",
      "pos_emb: torch.Size([36, 512])\n",
      "emb_out: torch.Size([128, 36, 512])\n",
      "\n",
      "-----------------------手撕position编码-----------------------\n",
      "位置编码向量tensor:  torch.Size([256, 512])\n",
      "pos: torch.Size([256])\n",
      "pos 增加一个维度后: torch.Size([256, 1])\n",
      "_2i  torch.Size([256])\n",
      "_2i[0:10]  tensor([ 0.,  2.,  4.,  6.,  8., 10., 12., 14., 16., 18.])\n",
      "赋值pos_embeding\n",
      "--------------: torch.Size([256, 512])\n",
      "打印前10个数据 tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000],\n",
      "        [ 0.8415,  0.5403,  0.8219,  0.5697,  0.8020],\n",
      "        [ 0.9093, -0.4161,  0.9364, -0.3509,  0.9581],\n",
      "        [ 0.1411, -0.9900,  0.2451, -0.9695,  0.3428],\n",
      "        [-0.7568, -0.6536, -0.6572, -0.7537, -0.5486]])\n",
      "128\n",
      "36\n",
      "torch.Size([36, 512])\n"
     ]
    }
   ],
   "source": [
    "# embedding 嵌入层计算\n",
    "# models/embedding/transformer_embedding.py\n",
    "# class TransformerEmbedding(nn.Module)\n",
    "\n",
    "emb = model.encoder.emb\n",
    "print(emb)\n",
    "tok_emb = emb.tok_emb(src)\n",
    "pos_emb = emb.pos_emb(src)\n",
    "emb_out = emb.drop_out(tok_emb + pos_emb)\n",
    "print('src:', src.shape)\n",
    "print('tok_emb:', tok_emb.shape)\n",
    "print('pos_emb:', pos_emb.shape)\n",
    "print('emb_out:', emb_out.shape)\n",
    "\n",
    "# tok_emb 使用 nn.embedding\n",
    "# pos_emb 计算如下\n",
    "# 512 / 2[cos/sin] -> i 256\n",
    "print('\\n-----------------------手撕position编码-----------------------')\n",
    "# 位置编码仅计算一次\n",
    "emb.pos_emb.encoding = torch.zeros(max_len, d_model)\n",
    "print(\"位置编码向量tensor: \",emb.pos_emb.encoding.shape)\n",
    "\n",
    "emb.pos_emb.encoding.requires_grad = False  # we don't need to compute gradient\n",
    "pos = torch.arange(0, max_len)\n",
    "print('pos:', pos.shape)\n",
    "pos = pos.float().unsqueeze(dim=1)\n",
    "print('pos 增加一个维度后:', pos.shape)\n",
    "\n",
    "_2i = torch.arange(0, d_model, step=2, device=device).float()\n",
    "print('_2i ', _2i.shape)\n",
    "print('_2i[0:10] ', _2i[:10])\n",
    "\n",
    "print('赋值pos_embeding')\n",
    "emb.pos_emb.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
    "emb.pos_emb.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
    "print('--------------:', emb.pos_emb.encoding.shape)\n",
    "print(\"打印前10个数据\", emb.pos_emb.encoding[0:5, 0:5])\n",
    "\n",
    "# 使用时\n",
    "batch_size, seq_len = src.size()\n",
    "print(batch_size)\n",
    "print(seq_len)\n",
    "print(emb.pos_emb.encoding[:seq_len, :].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "82bf543f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8943,  0.5794, -0.7674, -0.3635],\n",
      "        [-1.1186, -0.4357,  0.1521, -2.6122],\n",
      "        [-0.0393, -0.3453,  0.3196, -0.0532],\n",
      "        [ 0.2895,  1.9149,  0.6449, -0.2646],\n",
      "        [ 0.4749,  1.2114,  1.3111,  1.2232]])\n",
      "tensor([ 0.2895,  1.9149,  0.6449, -0.2646])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "PE = torch.randn(5, 4)\n",
    "\n",
    "print(PE)\n",
    "# print(PE[0,:])\n",
    "# print(PE[1,:])\n",
    "print(PE[3,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbcc170",
   "metadata": {},
   "source": [
    "### 3.4.3 计算 embedding->[encoder block]->output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3408496c",
   "metadata": {},
   "source": [
    "encoder block 主要包含multi-head attention 和 feed forward position两个主要模块\n",
    "\n",
    "![encoder_block](image/Transformer_encoder.png)\n",
    "\n",
    "----\n",
    "\n",
    "encoder block更加具体为\n",
    "\n",
    "![encoder_block detail](image/transformer_resideual_layer_norm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "77f35d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 请尝试独自debug各层类\n",
    "\n",
    "# 获取encode中的一个blocks\n",
    "layer=model.encoder.layers[0]\n",
    "# print(layer)\n",
    "\n",
    "# 0. 保留输入向量, 用于short cut\n",
    "emb_src = emb_out\n",
    "_emb_src = emb_src \n",
    "\n",
    "# 1. 编码层 多头-自注意力机制（后面会详细介绍）\n",
    "x = layer.attention(q=emb_src, k=emb_src, v=emb_src, mask=src_mask)\n",
    "\n",
    "# 2. dropout和layer-norm（后面会介绍）\n",
    "x = layer.dropout1(x)\n",
    "x = layer.norm1(x + _emb_src) # shorcut连接\n",
    "\n",
    "# 3. 基于位置的前向传播将维度512->2048->512\n",
    "_x = x\n",
    "x = layer.ffn(x)\n",
    "\n",
    "# 4. dropout + shortcut + layer-norm\n",
    "x = layer.dropout2(x)\n",
    "x = layer.norm2(x + _x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb119211",
   "metadata": {},
   "source": [
    "shortcut目的在以保留信息，防止信息损失，见ResNet\n",
    "\n",
    "![shortcut](image/transformer_resideual_layer_norm_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae7a56d",
   "metadata": {},
   "source": [
    "### 3.4.4 计算 embeding->[multi-head-attention]->score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a90d2b7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "输入输出\n",
    "![multi-head](image/transformer_attention_heads_z.png)\n",
    "\n",
    "\n",
    "------\n",
    "输出拼接\n",
    "![multi-concate](image/transformer_attention_heads_weight_matrix_o.png)\n",
    "\n",
    "-----\n",
    "Multi-head-attention计算流程\n",
    "![multi-head-attention-pipeline](image/transformer_multi-headed_self-attention-recap.png)\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "![wq](image/self-attention-matrix-calculation.png)\n",
    "---\n",
    "\n",
    "\n",
    "![8头](image/transformer_attention_heads_qkv.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8f1a487d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_head_attention层包含: MultiHeadAttention(\n",
      "  (attention): ScaleDotProductAttention(\n",
      "    (softmax): Softmax(dim=-1)\n",
      "  )\n",
      "  (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
      ")\n",
      "emb_src: torch.Size([128, 36, 512])\n",
      "x_attention_out: torch.Size([128, 36, 512])\n",
      "以下为多头注意力forward分解步骤：\n",
      "\n",
      " 0. 输入向量emb_src: torch.Size([128, 36, 512])\n",
      "q.shape: torch.Size([128, 36, 512])\n",
      "k.shape: torch.Size([128, 36, 512])\n",
      "v.shape: torch.Size([128, 36, 512])\n",
      "\n",
      " 1. 对qkv liner 转化\n",
      "q=f(q):  torch.Size([128, 36, 512])\n",
      "\n",
      " 2. 将输入向量拆成n_head\n",
      "n_heads: 8\n",
      "multi_head_attention.n_head: 8\n",
      "*-------multi_head_attention.split()-------------*\n",
      "d_model:512 / n_heads:8 = d_tensor:64\n",
      "单头向量维度为: 64\n",
      "_q_split: torch.Size([128, 8, 36, 64])\n",
      "*-------multi_head_attention.split()-------------*\n",
      "shape = [batch_size:128, heads:8, length:29, d_tensor:64]\n",
      "multi_head_attention.split(q): torch.Size([128, 8, 36, 64])\n",
      "multi_head_attention.split(k): torch.Size([128, 8, 36, 64])\n",
      "multi_head_attention.split(v): torch.Size([128, 8, 36, 64])\n",
      "\n",
      " 3. 计算单头注意力, scale and dot attention\n",
      "上面将512维度分成8头64维\n",
      "会独立介绍单头注意力的计算\n",
      "对每一头进行自注意力后的结果: torch.Size([128, 8, 36, 64])\n",
      "\n",
      " 4. 将8头64维拼接成512维度向量\n",
      "*-------multi_head_attention.concat()-------------*\n",
      "multi_head_attention.concat() 函数示例\n",
      "concat 操作后 torch.Size([128, 36, 512])\n",
      "*-------multi_head_attention.concat()-------------*\n",
      "after concat out shape: torch.Size([128, 36, 512])\n",
      "对多头注意力输出再进行前向传播 torch.Size([128, 36, 512])\n"
     ]
    }
   ],
   "source": [
    "# multihead多头注意力计算\n",
    "\n",
    "# encode multi-attention直接计算多头注意力分数\n",
    "multi_head_attention = model.encoder.layers[0].attention\n",
    "print(\"multi_head_attention层包含:\", multi_head_attention)\n",
    "x_attention_out = multi_head_attention(q=emb_src, k=emb_src, v=emb_src, mask=src_mask)\n",
    "print(\"emb_src:\", emb_src.shape)\n",
    "print(\"x_attention_out:\", x_attention_out.shape)\n",
    "print(\"以下为多头注意力forward分解步骤：\")\n",
    "\n",
    "# 0. 自注意力向量\n",
    "q = k = v = emb_src # embdedding+positional = x\n",
    "print(\"\\n 0. 输入向量emb_src:\", emb_src.shape)\n",
    "print(\"q.shape:\", q.shape)\n",
    "print(\"k.shape:\", k.shape)\n",
    "print(\"v.shape:\", v.shape)\n",
    "\n",
    "# 1. liner转化\n",
    "print(\"\\n 1. 对qkv liner 转化\")\n",
    "q = multi_head_attention.w_q(q)\n",
    "k = multi_head_attention.w_k(k)\n",
    "v = multi_head_attention.w_v(v)\n",
    "print(\"q=f(q): \", q.shape)\n",
    "\n",
    "# 2. 将输入向量拆成n_head\n",
    "print(\"\\n 2. 将输入向量拆成n_head\")\n",
    "print(\"n_heads:\", n_heads)\n",
    "print(\"multi_head_attention.n_head:\", multi_head_attention.n_head)\n",
    "_q = q\n",
    "\n",
    "# do split multi_head_attention.split()\n",
    "print('*-------multi_head_attention.split()-------------*')\n",
    "batch_size, length, d_model = _q.size()\n",
    "d_tensor = d_model // multi_head_attention.n_head\n",
    "print(\"d_model:{} / n_heads:{} = d_tensor:{}\".format(d_model, n_heads, d_tensor))\n",
    "print(\"单头向量维度为:\", d_tensor)\n",
    "_q_split = _q.view(batch_size, length, multi_head_attention.n_head, d_tensor).transpose(1, 2)\n",
    "print(\"_q_split:\", _q_split.shape)\n",
    "print('*-------multi_head_attention.split()-------------*')\n",
    "\n",
    "q, k, v = multi_head_attention.split(q), multi_head_attention.split(k), multi_head_attention.split(v)\n",
    "print(\"shape = [batch_size:128, heads:8, length:29, d_tensor:64]\")\n",
    "print(\"multi_head_attention.split(q):\", q.shape)\n",
    "print(\"multi_head_attention.split(k):\", k.shape)\n",
    "print(\"multi_head_attention.split(v):\", v.shape)\n",
    "\n",
    "\n",
    "# 3. do scale dot product to compute similarity\n",
    "# 计算每一头的attention（scale and dot attention）\n",
    "print(\"\\n 3. 计算单头注意力, scale and dot attention\")\n",
    "print(\"上面将512维度分成8头64维\")\n",
    "print(\"会独立介绍单头注意力的计算\")\n",
    "_q_single = q\n",
    "_k_single = k\n",
    "_v_single = v\n",
    "out, attention = multi_head_attention.attention(q, k, v, mask=src_mask)\n",
    "print(\"对每一头进行自注意力后的结果:\", out.shape)\n",
    "\n",
    "# 4. concat and pass to linear layer\n",
    "print(\"\\n 4. 将8头64维拼接成512维度向量\")\n",
    "_out = out \n",
    "# do concat \n",
    "\n",
    "print('*-------multi_head_attention.concat()-------------*')\n",
    "print(\"multi_head_attention.concat() 函数示例\")\n",
    "batch_size, head, length, d_tensor = _out.size()\n",
    "d_model = head * d_tensor\n",
    "_out_concat = _out.transpose(1, 2).contiguous().view(batch_size, length, d_model)\n",
    "print(\"concat 操作后\", _out_concat.shape)\n",
    "print('*-------multi_head_attention.concat()-------------*')\n",
    "\n",
    "out = multi_head_attention.concat(out)\n",
    "print(\"after concat out shape:\", out.shape)\n",
    "out = multi_head_attention.w_concat(out)\n",
    "print(\"对多头注意力输出再进行前向传播\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3157140",
   "metadata": {},
   "source": [
    "### 3.4.5 计算 [scale-dot-production] :  mask(q@k^t/scaled)@v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d63c6d",
   "metadata": {},
   "source": [
    "![pipeline](image/self-attention-matrix-calculation-2.png)\n",
    "![pipeline_qkv2](image/self-attention-output.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "448c76aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaleDotProductAttention(\n",
      "  (softmax): Softmax(dim=-1)\n",
      ")\n",
      "tensor中的格式： 只关注length句长， d_tensor向量长度\n",
      "[batch_size:128, head:8, length:36, d_tensor:64]\n",
      "q: torch.Size([128, 8, 36, 64])\n",
      "k_t: torch.Size([128, 8, 64, 36])\n",
      "通过计算两个向量的点积dot操作:score=q@k_t:  torch.Size([128, 8, 36, 36])\n",
      "每个词与词之间计算相关性\n",
      "score代表注意力分数 \n",
      "src_mask: torch.Size([128, 1, 36, 36])\n",
      "v: torch.Size([128, 8, 36, 64])\n",
      "score * v: torch.Size([128, 8, 36, 64])\n",
      "score * v: 代表注意力特征向量，即每个词在当前这个句子中的特征表达\n"
     ]
    }
   ],
   "source": [
    "# attention, 单头注意力计算\n",
    "# models/layer/scale_dot_product_attention.py\n",
    "# class ScaleDotProductAttention(nn.Module)\n",
    "\n",
    "attention = multi_head_attention.attention\n",
    "print(attention)\n",
    "\n",
    "# input is 4 dimension tensor\n",
    "# [batch_size, head, length, d_tensor]\n",
    "k = _k_single\n",
    "q = _q_single\n",
    "v = _v_single\n",
    "batch_size, head, length, d_tensor = k.size()\n",
    "\n",
    "print('tensor中的格式： 只关注length句长， d_tensor向量长度')\n",
    "print('[batch_size:{}, head:{}, length:{}, d_tensor:{}]'.format(batch_size,head,length,d_tensor))\n",
    "\n",
    "# 1. dot product Query with Key^T to compute similarity\n",
    "k_t = k.transpose(2, 3)  # transpose\n",
    "\n",
    "print(\"q:\", q.shape)\n",
    "print(\"k_t:\", k_t.shape)\n",
    "score = (q @ k_t) / math.sqrt(d_tensor)  # scaled dot product\n",
    "########   dot   ####### scaled #######\n",
    "print(\"通过计算两个向量的点积dot操作:score=q@k_t: \", score.shape)\n",
    "print(\"每个词与词之间计算相关性\")\n",
    "print(\"score代表注意力分数 \")\n",
    "print(\"src_mask:\", src_mask.shape)\n",
    "# 2. apply masking (opt)\n",
    "if src_mask is not None:\n",
    "    score = score.masked_fill(src_mask == 0, -10000)\n",
    "# 3. pass them softmax to make [0, 1] range\n",
    "score = attention.softmax(score)\n",
    "# 4. multiply with Value\n",
    "print(\"v:\", v.shape)\n",
    "v = score @ v\n",
    "print(\"score * v:\", v.shape)\n",
    "print(\"score * v: 代表注意力特征向量，即每个词在当前这个句子中的特征表达\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29fd07f",
   "metadata": {},
   "source": [
    "\n",
    "一个句子中：关于'it'单词的 单头自注意力score 30个词 [1,30,1] 'it'\n",
    "![vis-1](image/transformer_self-attention_visualization.png)\n",
    "\n",
    "一个句子中：关于'it'单词的 两头自注意力score 30个词 [2, 30,1] 'it'\n",
    "![vis-2](image/transformer_self-attention_visualization_2.png)\n",
    "\n",
    "\n",
    "一个句子中：关于'it'单词的 八头自注意力score 30个词 [8, 30,1] 'it'\n",
    "![vis-3](image/transformer_self-attention_visualization_3.png)\n",
    "\n",
    "\n",
    "一个句子中：关于30个单词的 八头自注意力score 30个词 [8, 30,30] \n",
    "\n",
    "128个句子中：关于30个单词的 八头自注意力score 30个词 [128, 8, 30,30] \n",
    "\n",
    "\n",
    "\n",
    "QK可视化 score\n",
    "![vis-gpt](image/gpt2-self-attention-scoring-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d25344b",
   "metadata": {},
   "source": [
    "### 3.4.6 计算 emb_src->[layer normaliztion] ->multihead attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebad6f11",
   "metadata": {},
   "source": [
    "layer norm 公式\n",
    "class LayerNorm(nn.Module)\n",
    "![layer](image/layer_norm.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b578cb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerNorm()\n",
      "==============LayerNorm===========\n",
      "LayerNorm gamma:  torch.Size([512])\n",
      "LayerNorm beta:  torch.Size([512])\n",
      "LayerNorm eps:  1e-12\n",
      "LayerNorm mean:  torch.Size([128, 36, 1])\n",
      "LayerNorm var:  torch.Size([128, 36, 1])\n",
      "LayerNorm norm out:  torch.Size([128, 36, 512])\n",
      "LayerNorm norm out offset:  torch.Size([128, 36, 512])\n"
     ]
    }
   ],
   "source": [
    "# Layer Normalization, 层归一化\n",
    "# models/layer/layer_norm.py\n",
    "# class LayerNorm(nn.Module)\n",
    "\n",
    "norm = model.encoder.layers[0].norm1\n",
    "print(norm)\n",
    "\n",
    "x = emb_src\n",
    "print(\"==============LayerNorm===========\")\n",
    "print(\"LayerNorm gamma: \", norm.gamma.shape)\n",
    "print(\"LayerNorm beta: \", norm.beta.shape)\n",
    "print(\"LayerNorm eps: \", norm.eps)\n",
    "\n",
    "mean = x.mean(-1, keepdim=True)\n",
    "print(\"LayerNorm mean: \", mean.shape)\n",
    "\n",
    "var = x.var(-1, unbiased=False, keepdim=True)\n",
    "print(\"LayerNorm var: \", var.shape)\n",
    "# '-1' means last dimension. \n",
    "\n",
    "out = (x - mean) / torch.sqrt(var + norm.eps)\n",
    "print(\"LayerNorm norm out: \", out.shape)\n",
    "\n",
    "out = norm.gamma * out + norm.beta\n",
    "print(\"LayerNorm norm out offset: \", out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8a81da",
   "metadata": {},
   "source": [
    "### 3.4.7 计算attention-> [position-wise-feed-forward]->layernorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0cc7edd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PositionwiseFeedForward, 位置前向传播\n",
      "PositionwiseFeedForward(\n",
      "  (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "n_hidden:  2048\n",
      "1. before linear: torch.Size([128, 36, 512])\n",
      "2. after linear1: torch.Size([128, 36, 2048])\n",
      "3. after linear2: torch.Size([128, 36, 512])\n"
     ]
    }
   ],
   "source": [
    "# PositionwiseFeedForward, 位置前向传播\n",
    "# models/layer/position_wise_feed_forward.py\n",
    "# class PositionwiseFeedForward(nn.Module)\n",
    "print(\"PositionwiseFeedForward, 位置前向传播\")\n",
    "ffn = model.encoder.layers[0].ffn\n",
    "print(ffn)\n",
    "print(\"n_hidden: \", ffn_hidden)\n",
    "\n",
    "_x = emb_src\n",
    "print(\"1. before linear:\", _x.shape)\n",
    "\n",
    "_x = ffn.linear1(_x)\n",
    "print(\"2. after linear1:\", _x.shape)\n",
    "\n",
    "_x = ffn.relu(_x)\n",
    "_x = ffn.dropout(_x)\n",
    "_x = ffn.linear2(_x)\n",
    "print(\"3. after linear2:\", _x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a854423",
   "metadata": {},
   "source": [
    "### 3.4.8 计算enc_src+emb_trg->[decoder]->output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef53544",
   "metadata": {},
   "source": [
    "![decoder_dataflow_block](image/The_transformer_encoder_decoder_stack.png)\n",
    "![decoder_dataflow](image/transformer_resideual_layer_norm_3.png)\n",
    "![decoder_pipeline_single](image/transformer_decoding_2.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ecdfc33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "解码层结构：\n",
      "解码层输入target和编码层一样做embeding\n",
      "trg输入 torch.Size([128, 38])\n",
      "trg embding torch.Size([128, 38, 512])\n",
      "解码层数: 6\n",
      "编码层输出： torch.Size([128, 38, 512])\n",
      "编码层liner处理： torch.Size([128, 38, 7853])\n"
     ]
    }
   ],
   "source": [
    "# Decoder, 解码结构\n",
    "# models/model/decoder.py\n",
    "# class Decoder(nn.Module)\n",
    "print(\"解码层结构：\")\n",
    "# print(model.decoder)\n",
    "\n",
    "print(\"解码层输入target和编码层一样做embeding\")\n",
    "print(\"trg输入\", trg.shape)\n",
    "emb_trg = model.decoder.emb(trg) # target -> Label mask\n",
    "print(\"trg embding\", emb_trg.shape)\n",
    "print(\"解码层数:\", len(model.decoder.layers))\n",
    "\n",
    "# encoder - > encoder K encoder V\n",
    "# decoder Q\n",
    "\n",
    "for layer in model.decoder.layers:\n",
    "    # 注意这里需要有编码层的输入\n",
    "    decode_trg = layer(emb_trg, enc_src, trg_mask, src_trg_mask)\n",
    "print(\"编码层输出：\", decode_trg.shape)\n",
    "# pass to LM head\n",
    "output_decode = model.decoder.linear(decode_trg)\n",
    "print(\"编码层liner处理：\", output_decode.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484c43f6",
   "metadata": {},
   "source": [
    "### 3.4.9 计算[decoder block]: decode-self-attention -> enc-dec-attention ->ffn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a86d9f",
   "metadata": {},
   "source": [
    "![encoder-decoder](image/transformer_resideual_layer_norm_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a0a89454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: trg_x: torch.Size([128, 38, 512])\n",
      "k: enc: torch.Size([128, 36, 512])\n",
      "v: enc: torch.Size([128, 36, 512])\n",
      "mask: src_trg_mask: torch.Size([128, 1, 38, 36])\n",
      "enc->dec 注意力后:  torch.Size([128, 38, 512])\n"
     ]
    }
   ],
   "source": [
    "# DecoderLayer, 解码层\n",
    "# models/blocks/decoder_layer.py\n",
    "# class DecoderLayer(nn.Module)\n",
    "\n",
    "# decode layer\n",
    "layer = model.decoder.layers[0]\n",
    "# print(\"decode layer结构：\")\n",
    "# print(layer)\n",
    "\n",
    "dec = emb_trg\n",
    "enc = enc_src\n",
    "_x = dec\n",
    "\n",
    "x = layer.self_attention(q=dec, k=dec, v=dec, mask=trg_mask)\n",
    "x = layer.dropout1(x)\n",
    "x = layer.norm1(x + _x)\n",
    "\n",
    "if enc is not None:\n",
    "    # 3. compute encoder - decoder attention\n",
    "    _x = x\n",
    "    # 多头注意力机制\n",
    "    print('q: trg_x:', x.shape)\n",
    "    print('k: enc:', enc.shape)\n",
    "    print('v: enc:', enc.shape)\n",
    "    print('mask: src_trg_mask:', src_trg_mask.shape)\n",
    "    x = layer.enc_dec_attention(q=x, k=enc, v=enc, mask=src_trg_mask)\n",
    "    print(\"enc->dec 注意力后: \", x.shape)\n",
    "    # 4. add and norm\n",
    "    x = layer.dropout2(x)\n",
    "    x = layer.norm2(x + _x)\n",
    "\n",
    "# 5. positionwise feed forward network\n",
    "_x = x\n",
    "x = layer.ffn(x)\n",
    "\n",
    "# 6. add and norm\n",
    "x = layer.dropout3(x)\n",
    "x = layer.norm3(x + _x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f091757",
   "metadata": {},
   "source": [
    "### 3.4.10 计算loss : output->[Cross Entropy loss]->logits->loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec5ade5",
   "metadata": {},
   "source": [
    "![loss](image/transformer_decoder_output_softmax.png)\n",
    "![loss_vocab](image/output_trained_model_probability_distributions.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "24281c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "损失计算，使用交叉损失：\n",
      "src: torch.Size([128, 36])\n",
      "trg: torch.Size([128, 38])\n",
      "trg[:, :-1]: torch.Size([128, 37])\n",
      "output: torch.Size([128, 37, 7853])\n",
      "output_reshape: torch.Size([4736, 7853])\n",
      "trg.view(-1): torch.Size([4736])\n",
      "loss: tensor(10.3012, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## 损失计算\n",
    "print('损失计算，使用交叉损失：')\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=src_pad_idx)\n",
    "print('src:', src.shape)\n",
    "print('trg:', trg.shape)\n",
    "print('trg[:, :-1]:', trg[:, :-1].shape)\n",
    "output = model(src, trg[:, :-1])\n",
    "print('output:', output.shape)\n",
    "output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "print('output_reshape:', output_reshape.shape)\n",
    "\n",
    "trg_view = trg[:, 1:].contiguous().view(-1)\n",
    "print('trg.view(-1):', trg_view.shape)\n",
    "loss = criterion(output_reshape, trg_view)\n",
    "print('loss:', loss)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886ffa35",
   "metadata": {},
   "source": [
    "### 3.4.11 编解码Mask计算原理enc-dec-mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "edaa9891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_mask: torch.Size([128, 1, 36, 36])\n",
      "src_trg_mask: torch.Size([128, 1, 38, 36])\n",
      "trg_mask: torch.Size([128, 1, 38, 38])\n",
      "tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1]], dtype=torch.int32)\n",
      "tensor([[1, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1]], dtype=torch.int32)\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "## mask机制\n",
    "print(\"src_mask:\", src_mask.shape)\n",
    "print(\"src_trg_mask:\", src_trg_mask.shape)\n",
    "print(\"trg_mask:\", trg_mask.shape)\n",
    "# print(src_mask[0][0].int())\n",
    "# print(src_trg_mask[0][0].int())\n",
    "print(src_mask[0,0,:5,:5].int())\n",
    "print(trg_mask[0,0,:5,:5].int())\n",
    "print(src_trg_mask[0,0,:20,:20].int())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "85cefe62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: torch.Size([128, 8, 38, 64])\n",
      "k_t: torch.Size([128, 8, 64, 36])\n",
      "score: torch.Size([128, 8, 38, 36])\n",
      "enc-dec-mask: torch.Size([128, 1, 38, 36])\n",
      "v: torch.Size([128, 8, 36, 64])\n",
      "score * v: torch.Size([128, 8, 38, 64])\n"
     ]
    }
   ],
   "source": [
    "# encode-decode-mask\n",
    "layer = model.decoder.layers[0]\n",
    "# print(\"decode layer结构：\")\n",
    "# print(layer)\n",
    "\n",
    "dec = emb_trg\n",
    "enc = enc_src\n",
    "\n",
    "# _x = dec\n",
    "# 1. decode self attention for target \n",
    "# x = layer.self_attention(q=dec, k=dec, v=dec, mask=trg_mask)\n",
    "# x = layer.dropout1(x)\n",
    "# x = layer.norm1(x + _x)\n",
    "\n",
    "# 2. ecode-decode-attention + mask\n",
    "# if enc is not None:\n",
    "#     # 3. compute encoder - decoder attention\n",
    "#     _x = x\n",
    "#     # 多头注意力机制\n",
    "#     print('q: trg_x:', x.shape)\n",
    "#     print('k: enc:', enc.shape)\n",
    "#     print('v: enc:', enc.shape)\n",
    "#     print('mask: src_trg_mask:', src_trg_mask.shape)\n",
    "#     x = layer.enc_dec_attention(q=x, k=enc, v=enc, mask=src_trg_mask)\n",
    "\n",
    "# layer.enc_dec_attention 多头\n",
    "# layer.enc_dec_attention.attention() 单头\n",
    "\n",
    "\n",
    "q_dec = dec\n",
    "q = q_dec = layer.enc_dec_attention.split(dec)\n",
    "k = k_enc = _k_single\n",
    "v = v_enc = _v_single\n",
    "\n",
    "batch_size, head, length, d_tensor = k.size()\n",
    "\n",
    "# 1. dot product Query with Key^T to compute similarity\n",
    "k_t = k.transpose(2, 3)  # transpose\n",
    "\n",
    "\n",
    "print(\"q:\", q.shape)\n",
    "print(\"k_t:\", k_t.shape)\n",
    "score = (q @ k_t) / math.sqrt(d_tensor)  # scaled dot product\n",
    "print(\"score:\", score.shape)\n",
    "# 2. apply masking (opt)\n",
    "if src_trg_mask is not None: # 实际预测时，没有mask，会预测出终止标志符号\n",
    "    print(\"enc-dec-mask:\",src_trg_mask.shape)\n",
    "    score = score.masked_fill(src_trg_mask == 0, -10000)\n",
    "# 3. pass them softmax to make [0, 1] range\n",
    "score = attention.softmax(score)\n",
    "# 4. multiply with Value\n",
    "print(\"v:\", v.shape)\n",
    "v = score @ v\n",
    "print(\"score * v:\", v.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47227947",
   "metadata": {},
   "source": [
    "## 4. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "aabb62dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(params=model.parameters(),\n",
    "                 lr=init_lr,\n",
    "                 weight_decay=weight_decay,\n",
    "                 eps=adam_eps)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 verbose=True,\n",
    "                                                 factor=factor,\n",
    "                                                 patience=patience)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=src_pad_idx)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src \n",
    "        trg = batch.trg \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg[:, :-1])\n",
    "        output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "        trg = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(output_reshape, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        print('step :', round((i / len(iterator)) * 100, 2), '% , loss :', loss.item())\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8eee504c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_iter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m train_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iter_max):\n\u001b[0;32m----> 5\u001b[0m         train_loss \u001b[38;5;241m=\u001b[39m train(model, \u001b[43mtrain_iter\u001b[49m, optimizer, criterion, clip)\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m>\u001b[39m warmup:\n\u001b[1;32m      8\u001b[0m             scheduler\u001b[38;5;241m.\u001b[39mstep(valid_loss)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_iter' is not defined"
     ]
    }
   ],
   "source": [
    "iter_max = 100\n",
    "# iter_max = 1000\n",
    "train_losses = []\n",
    "for step in range(iter_max):\n",
    "        train_loss = train(model, train_iter, optimizer, criterion, clip)\n",
    "\n",
    "        if step > warmup:\n",
    "            scheduler.step(valid_loss)\n",
    "        train_losses.append(train_loss)\n",
    "        f = open('result/train_loss.txt', 'w')\n",
    "        f.write(str(train_losses))\n",
    "        f.close()\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "torch.save(model.state_dict(), 'model-final.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97565fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "print(train_losses)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(train_losses, 'r', label='train')\n",
    "plt.title('training result')\n",
    "plt.grid(True, which='both', axis='both')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
